{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84edf436",
   "metadata": {},
   "source": [
    "# Vibe Coding by QIU Feng\n",
    "\n",
    "ä¸€ä¸ªå®Œæ•´çš„ **Vibe Coding** Jupyter Notebookï¼è¿™ä¸ª Notebook åŒ…å«ï¼š\n",
    "\n",
    "## ğŸ¯ ä¸»è¦ç‰¹è‰²ï¼š\n",
    "\n",
    "1. **ğŸ“Š æ•°æ®å¯è§†åŒ–ç»„åˆ** - Pandas + Matplotlib + Seaborn\n",
    "2. **ğŸ¤– æœºå™¨å­¦ä¹ ç¤ºä¾‹** - TensorFlow æˆ¿ä»·é¢„æµ‹æ¨¡å‹\n",
    "3. **ğŸ•·ï¸ ç½‘é¡µæŠ“å–æ¼”ç¤º** - BeautifulSoup æ•°æ®æå–\n",
    "4. **ğŸ–¼ï¸ è®¡ç®—æœºè§†è§‰** - OpenCV å›¾åƒå¤„ç†\n",
    "5. **ğŸš€ API å¼€å‘ä»£ç ** - FastAPI ç¤ºä¾‹\n",
    "6. **ğŸ® æ¸¸æˆå¼€å‘ä»£ç ** - Pygame ç¤ºä¾‹\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4f5efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas matplotlib seaborn beautifulsoup4 requests\n",
    "# pip install tensorflow scikit-learn opencv-python\n",
    "# pip install fastapi uvicorn pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d49dc4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1. ğŸ“Š Pandas + Matplotlib = æ•°æ®å¤„ç†ä¸å¯è§†åŒ–\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0735d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# è®¾ç½®matplotlibä¸­æ–‡æ˜¾ç¤º\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"ğŸ”¹ åˆ›å»ºæ¨¡æ‹Ÿç”µå•†é”€å”®æ•°æ®\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# åˆ›å»ºç¤ºä¾‹æ•°æ® - æ¨¡æ‹Ÿ6ä¸ªæœˆçš„ç”µå•†é”€å”®æƒ…å†µ\n",
    "data = {\n",
    "    'æœˆä»½': ['1æœˆ', '2æœˆ', '3æœˆ', '4æœˆ', '5æœˆ', '6æœˆ'],\n",
    "    'é”€å”®é¢': [120, 135, 148, 162, 180, 195],  # å•ä½ï¼šä¸‡å…ƒ\n",
    "    'å®¢æˆ·æ•°': [45, 52, 58, 64, 72, 78],        # å•ä½ï¼šä¸‡äºº\n",
    "    'è®¢å•æ•°': [380, 420, 465, 510, 580, 620]   # å•ä½ï¼šä¸‡å•\n",
    "}\n",
    "\n",
    "# ä½¿ç”¨Pandasåˆ›å»ºDataFrameè¿›è¡Œæ•°æ®å¤„ç†\n",
    "df = pd.DataFrame(data)\n",
    "print(\"åŸå§‹æ•°æ®é¢„è§ˆï¼š\")\n",
    "print(df)\n",
    "print(\"\\næ•°æ®ç»Ÿè®¡æ‘˜è¦ï¼š\")\n",
    "print(df.describe())\n",
    "\n",
    "# è®¡ç®—å¢é•¿ç‡\n",
    "df['é”€å”®é¢å¢é•¿ç‡'] = df['é”€å”®é¢'].pct_change() * 100\n",
    "df['å®¢æˆ·å¢é•¿ç‡'] = df['å®¢æˆ·æ•°'].pct_change() * 100\n",
    "\n",
    "print(\"\\nå¢é•¿ç‡åˆ†æï¼š\")\n",
    "print(df[['æœˆä»½', 'é”€å”®é¢å¢é•¿ç‡', 'å®¢æˆ·å¢é•¿ç‡']].round(2))\n",
    "\n",
    "# ä½¿ç”¨Matplotlibè¿›è¡Œæ•°æ®å¯è§†åŒ–\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('ğŸ“Š ç”µå•†ä¸šåŠ¡æ•°æ®åˆ†æçœ‹æ¿', fontsize=16, fontweight='bold')\n",
    "\n",
    "# å­å›¾1ï¼šé”€å”®é¢è¶‹åŠ¿çº¿å›¾\n",
    "axes[0,0].plot(df['æœˆä»½'], df['é”€å”®é¢'], marker='o', linewidth=2, markersize=8, color='#2E86AB')\n",
    "axes[0,0].set_title('ğŸ“ˆ æœˆåº¦é”€å”®é¢è¶‹åŠ¿', fontsize=12, fontweight='bold')\n",
    "axes[0,0].set_ylabel('é”€å”®é¢ (ä¸‡å…ƒ)')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "for i, v in enumerate(df['é”€å”®é¢']):\n",
    "    axes[0,0].annotate(f'{v}ä¸‡', (i, v), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "# å­å›¾2ï¼šå®¢æˆ·æ•°æŸ±çŠ¶å›¾\n",
    "bars = axes[0,1].bar(df['æœˆä»½'], df['å®¢æˆ·æ•°'], color='#A23B72', alpha=0.8)\n",
    "axes[0,1].set_title('ğŸ‘¥ æœˆåº¦å®¢æˆ·æ•°ç»Ÿè®¡', fontsize=12, fontweight='bold')\n",
    "axes[0,1].set_ylabel('å®¢æˆ·æ•° (ä¸‡äºº)')\n",
    "axes[0,1].grid(True, alpha=0.3, axis='y')\n",
    "for bar, value in zip(bars, df['å®¢æˆ·æ•°']):\n",
    "    axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                   f'{value}ä¸‡', ha='center', va='bottom')\n",
    "\n",
    "# å­å›¾3ï¼šè®¢å•æ•°é¢ç§¯å›¾\n",
    "axes[1,0].fill_between(df['æœˆä»½'], df['è®¢å•æ•°'], alpha=0.6, color='#F18F01')\n",
    "axes[1,0].plot(df['æœˆä»½'], df['è®¢å•æ•°'], marker='s', color='#C73E1D', linewidth=2)\n",
    "axes[1,0].set_title('ğŸ“¦ æœˆåº¦è®¢å•æ•°è¶‹åŠ¿', fontsize=12, fontweight='bold')\n",
    "axes[1,0].set_ylabel('è®¢å•æ•° (ä¸‡å•)')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# å­å›¾4ï¼šå¤šæŒ‡æ ‡å¯¹æ¯”\n",
    "x = np.arange(len(df['æœˆä»½']))\n",
    "width = 0.25\n",
    "\n",
    "axes[1,1].bar(x - width, df['é”€å”®é¢']/10, width, label='é”€å”®é¢(åä¸‡å…ƒ)', color='#2E86AB', alpha=0.8)\n",
    "axes[1,1].bar(x, df['å®¢æˆ·æ•°'], width, label='å®¢æˆ·æ•°(ä¸‡äºº)', color='#A23B72', alpha=0.8)\n",
    "axes[1,1].bar(x + width, df['è®¢å•æ•°']/10, width, label='è®¢å•æ•°(åä¸‡å•)', color='#F18F01', alpha=0.8)\n",
    "\n",
    "axes[1,1].set_title('ğŸ“Š å¤šæŒ‡æ ‡å¯¹æ¯”åˆ†æ', fontsize=12, fontweight='bold')\n",
    "axes[1,1].set_ylabel('æ•°å€¼')\n",
    "axes[1,1].set_xticks(x)\n",
    "axes[1,1].set_xticklabels(df['æœˆä»½'])\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# æ•°æ®æ´å¯Ÿåˆ†æ\n",
    "print(\"\\nğŸ” æ•°æ®æ´å¯Ÿï¼š\")\n",
    "print(f\"â€¢ æ€»é”€å”®é¢ï¼š{df['é”€å”®é¢'].sum()}ä¸‡å…ƒ\")\n",
    "print(f\"â€¢ å¹³å‡æœˆé”€å”®é¢ï¼š{df['é”€å”®é¢'].mean():.1f}ä¸‡å…ƒ\")\n",
    "print(f\"â€¢ é”€å”®é¢å¢é•¿ï¼š{((df['é”€å”®é¢'].iloc[-1] - df['é”€å”®é¢'].iloc[0]) / df['é”€å”®é¢'].iloc[0] * 100):.1f}%\")\n",
    "print(f\"â€¢ å®¢æˆ·æ•°å¢é•¿ï¼š{((df['å®¢æˆ·æ•°'].iloc[-1] - df['å®¢æˆ·æ•°'].iloc[0]) / df['å®¢æˆ·æ•°'].iloc[0] * 100):.1f}%\")\n",
    "print(f\"â€¢ å¹³å‡å®¢å•ä»·ï¼š{(df['é”€å”®é¢'] * 10000 / (df['å®¢æˆ·æ•°'] * 10000)).mean():.0f}å…ƒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43f51e5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. ğŸ¨ Seaborn = é«˜çº§ç»Ÿè®¡å›¾è¡¨\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adab6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # è®¾ç½®ä¸­æ–‡å­—ä½“ä¸ºé»‘ä½“\n",
    "plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜\n",
    "\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸ”¹ ä½¿ç”¨Seabornåˆ›å»ºé«˜çº§ç»Ÿè®¡å›¾è¡¨\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# è®¾ç½®Seabornæ ·å¼\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# åˆ›å»ºæ›´å¤æ‚çš„äº§å“é”€å”®æ•°æ®é›†\n",
    "np.random.seed(42)  # ç¡®ä¿ç»“æœå¯é‡ç°\n",
    "n_samples = 200\n",
    "\n",
    "print(\"ğŸ“Š ç”Ÿæˆæ¨¡æ‹Ÿäº§å“æ•°æ®é›†...\")\n",
    "\n",
    "# æ¨¡æ‹Ÿäº§å“æ•°æ®\n",
    "categories = ['ç”µå­äº§å“', 'æœè£…é…é¥°', 'å®¶å±…ç”¨å“', 'å›¾ä¹¦éŸ³åƒ', 'é£Ÿå“é¥®æ–™']\n",
    "brands = ['å“ç‰ŒA', 'å“ç‰ŒB', 'å“ç‰ŒC', 'å“ç‰ŒD', 'å“ç‰ŒE']\n",
    "regions = ['ååŒ—', 'åä¸œ', 'åå—', 'åä¸­', 'è¥¿å—']\n",
    "\n",
    "# åˆ›å»ºå¤æ‚çš„å…³è”æ•°æ®\n",
    "complex_data = pd.DataFrame({\n",
    "    'äº§å“ç±»åˆ«': np.random.choice(categories, n_samples),\n",
    "    'å“ç‰Œ': np.random.choice(brands, n_samples),\n",
    "    'é”€å”®åŒºåŸŸ': np.random.choice(regions, n_samples),\n",
    "    'ä»·æ ¼': np.random.gamma(2, 50, n_samples),  # ä¼½é©¬åˆ†å¸ƒï¼Œæ›´çœŸå®çš„ä»·æ ¼åˆ†å¸ƒ\n",
    "    'è¯„åˆ†': np.random.beta(8, 2, n_samples) * 4 + 1,  # Betaåˆ†å¸ƒï¼Œè¯„åˆ†é›†ä¸­åœ¨é«˜åˆ†\n",
    "    'é”€é‡': np.random.poisson(25, n_samples),  # æ³Šæ¾åˆ†å¸ƒï¼Œé”€é‡æ•°æ®\n",
    "    'ä¿ƒé”€æ´»åŠ¨': np.random.choice(['æ˜¯', 'å¦'], n_samples, p=[0.3, 0.7])\n",
    "})\n",
    "\n",
    "# æ·»åŠ ä¸€äº›ç°å®çš„å…³è”æ€§\n",
    "# ç”µå­äº§å“ä»·æ ¼é€šå¸¸æ›´é«˜\n",
    "mask_electronics = complex_data['äº§å“ç±»åˆ«'] == 'ç”µå­äº§å“'\n",
    "complex_data.loc[mask_electronics, 'ä»·æ ¼'] *= 2\n",
    "\n",
    "# ä»·æ ¼å½±å“é”€é‡ï¼ˆè´Ÿç›¸å…³ï¼‰\n",
    "price_effect = 1 - (complex_data['ä»·æ ¼'] - complex_data['ä»·æ ¼'].min()) / (complex_data['ä»·æ ¼'].max() - complex_data['ä»·æ ¼'].min()) * 0.5\n",
    "complex_data['é”€é‡'] = (complex_data['é”€é‡'] * price_effect).astype(int)\n",
    "\n",
    "# ä¿ƒé”€æ´»åŠ¨å½±å“é”€é‡\n",
    "promotion_mask = complex_data['ä¿ƒé”€æ´»åŠ¨'] == 'æ˜¯'\n",
    "complex_data.loc[promotion_mask, 'é”€é‡'] *= 1.5\n",
    "\n",
    "print(f\"æ•°æ®é›†å¤§å°ï¼š{len(complex_data)} æ¡è®°å½•\")\n",
    "print(\"\\næ•°æ®é¢„è§ˆï¼š\")\n",
    "print(complex_data.head())\n",
    "print(\"\\nå„ç±»åˆ«æ•°é‡åˆ†å¸ƒï¼š\")\n",
    "print(complex_data['äº§å“ç±»åˆ«'].value_counts())\n",
    "\n",
    "# åˆ›å»ºé«˜çº§ç»Ÿè®¡å›¾è¡¨\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 15))\n",
    "fig.suptitle('ğŸ¨ Seaborné«˜çº§ç»Ÿè®¡å›¾è¡¨åˆ†æ', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. ç®±çº¿å›¾ - å„ç±»åˆ«äº§å“ä»·æ ¼åˆ†å¸ƒ\n",
    "sns.boxplot(data=complex_data, x='äº§å“ç±»åˆ«', y='ä»·æ ¼', ax=axes[0,0])\n",
    "axes[0,0].set_title('ğŸ“¦ å„ç±»åˆ«äº§å“ä»·æ ¼åˆ†å¸ƒç®±çº¿å›¾', fontsize=12, fontweight='bold')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "axes[0,0].set_ylabel('ä»·æ ¼ (å…ƒ)')\n",
    "\n",
    "# 2. æ•£ç‚¹å›¾ - ä»·æ ¼ä¸è¯„åˆ†å…³ç³»\n",
    "sns.scatterplot(data=complex_data, x='ä»·æ ¼', y='è¯„åˆ†', hue='äº§å“ç±»åˆ«', \n",
    "                size='é”€é‡', sizes=(50, 200), alpha=0.7, ax=axes[0,1])\n",
    "axes[0,1].set_title('ğŸ’ ä»·æ ¼ä¸è¯„åˆ†å…³ç³»æ•£ç‚¹å›¾', fontsize=12, fontweight='bold')\n",
    "axes[0,1].set_xlabel('ä»·æ ¼ (å…ƒ)')\n",
    "axes[0,1].set_ylabel('è¯„åˆ†')\n",
    "\n",
    "# 3. çƒ­åŠ›å›¾ - æ•°å€¼å˜é‡ç›¸å…³æ€§\n",
    "numeric_data = complex_data.select_dtypes(include=[np.number])\n",
    "correlation_matrix = numeric_data.corr()\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='RdYlBu_r', \n",
    "            center=0, square=True, ax=axes[1,0])\n",
    "axes[1,0].set_title('ğŸ”¥ æ•°æ®ç›¸å…³æ€§çƒ­åŠ›å›¾', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 4. å°æç´å›¾ - å„åŒºåŸŸé”€é‡åˆ†å¸ƒ\n",
    "sns.violinplot(data=complex_data, x='é”€å”®åŒºåŸŸ', y='é”€é‡', ax=axes[1,1])\n",
    "axes[1,1].set_title('ğŸ» å„åŒºåŸŸé”€é‡åˆ†å¸ƒå°æç´å›¾', fontsize=12, fontweight='bold')\n",
    "axes[1,1].set_ylabel('é”€é‡ (ä»¶)')\n",
    "\n",
    "# 5. åˆ†ç»„æŸ±çŠ¶å›¾ - ä¿ƒé”€æ´»åŠ¨æ•ˆæœ\n",
    "promotion_effect = complex_data.groupby(['äº§å“ç±»åˆ«', 'ä¿ƒé”€æ´»åŠ¨'])['é”€é‡'].mean().reset_index()\n",
    "sns.barplot(data=promotion_effect, x='äº§å“ç±»åˆ«', y='é”€é‡', hue='ä¿ƒé”€æ´»åŠ¨', ax=axes[2,0])\n",
    "axes[2,0].set_title('ğŸ“Š ä¿ƒé”€æ´»åŠ¨å¯¹é”€é‡å½±å“', fontsize=12, fontweight='bold')\n",
    "axes[2,0].tick_params(axis='x', rotation=45)\n",
    "axes[2,0].set_ylabel('å¹³å‡é”€é‡ (ä»¶)')\n",
    "\n",
    "# 6. åˆ†å¸ƒå›¾ - ä»·æ ¼åˆ†å¸ƒ\n",
    "sns.histplot(data=complex_data, x='ä»·æ ¼', hue='äº§å“ç±»åˆ«', \n",
    "             multiple=\"stack\", alpha=0.7, ax=axes[2,1])\n",
    "axes[2,1].set_title('ğŸ“ˆ å„ç±»åˆ«äº§å“ä»·æ ¼åˆ†å¸ƒ', fontsize=12, fontweight='bold')\n",
    "axes[2,1].set_xlabel('ä»·æ ¼ (å…ƒ)')\n",
    "axes[2,1].set_ylabel('æ•°é‡')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ç»Ÿè®¡åˆ†ææŠ¥å‘Š\n",
    "print(\"\\nğŸ“‹ ç»Ÿè®¡åˆ†ææŠ¥å‘Šï¼š\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ä»·æ ¼ç»Ÿè®¡\n",
    "price_stats = complex_data.groupby('äº§å“ç±»åˆ«')['ä»·æ ¼'].agg(['mean', 'std', 'min', 'max'])\n",
    "print(\"å„ç±»åˆ«ä»·æ ¼ç»Ÿè®¡ï¼š\")\n",
    "print(price_stats.round(2))\n",
    "\n",
    "# é”€é‡ç»Ÿè®¡\n",
    "sales_stats = complex_data.groupby('äº§å“ç±»åˆ«')['é”€é‡'].agg(['mean', 'std', 'sum'])\n",
    "print(\"\\nå„ç±»åˆ«é”€é‡ç»Ÿè®¡ï¼š\")\n",
    "print(sales_stats.round(2))\n",
    "\n",
    "# ä¿ƒé”€æ•ˆæœåˆ†æ\n",
    "promotion_analysis = complex_data.groupby('ä¿ƒé”€æ´»åŠ¨').agg({\n",
    "    'é”€é‡': ['mean', 'sum'],\n",
    "    'ä»·æ ¼': 'mean',\n",
    "    'è¯„åˆ†': 'mean'\n",
    "}).round(2)\n",
    "print(\"\\nä¿ƒé”€æ´»åŠ¨æ•ˆæœåˆ†æï¼š\")\n",
    "print(promotion_analysis)\n",
    "\n",
    "# ç›¸å…³æ€§åˆ†æ\n",
    "print(f\"\\nğŸ” å…³é”®å‘ç°ï¼š\")\n",
    "print(f\"â€¢ ä»·æ ¼ä¸é”€é‡ç›¸å…³ç³»æ•°ï¼š{correlation_matrix.loc['ä»·æ ¼', 'é”€é‡']:.3f}\")\n",
    "print(f\"â€¢ è¯„åˆ†ä¸é”€é‡ç›¸å…³ç³»æ•°ï¼š{correlation_matrix.loc['è¯„åˆ†', 'é”€é‡']:.3f}\")\n",
    "print(f\"â€¢ ä¿ƒé”€æ´»åŠ¨å¹³å‡æå‡é”€é‡ï¼š{(complex_data[complex_data['ä¿ƒé”€æ´»åŠ¨']=='æ˜¯']['é”€é‡'].mean() - complex_data[complex_data['ä¿ƒé”€æ´»åŠ¨']=='å¦']['é”€é‡'].mean()):.1f}ä»¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d713f7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. ğŸ•·ï¸ BeautifulSoup = ç½‘é¡µæ•°æ®æŠ“å–\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0d59a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "print(\"ğŸ”¹ ä½¿ç”¨BeautifulSoupè¿›è¡Œç½‘é¡µæ•°æ®æŠ“å–\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# æ¨¡æ‹Ÿç½‘é¡µæŠ“å–ç¤ºä¾‹ï¼ˆä½¿ç”¨æœ¬åœ°HTMLï¼‰\n",
    "# åˆ›å»ºä¸€ä¸ªæ›´å¤æ‚çš„HTMLç¤ºä¾‹ï¼Œæ¨¡æ‹Ÿç”µå•†ç½‘ç«™\n",
    "sample_html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Vibeå•†åŸ - æœ€æ–°äº§å“</title>\n",
    "    <meta charset=\"UTF-8\">\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"header\">\n",
    "        <h1>Vibeå•†åŸ</h1>\n",
    "        <nav>\n",
    "            <ul>\n",
    "                <li><a href=\"/electronics\">ç”µå­äº§å“</a></li>\n",
    "                <li><a href=\"/books\">å›¾ä¹¦</a></li>\n",
    "                <li><a href=\"/clothing\">æœè£…</a></li>\n",
    "            </ul>\n",
    "        </nav>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"products-container\">\n",
    "        <div class=\"product\" data-category=\"electronics\">\n",
    "            <img src=\"laptop.jpg\" alt=\"ç¬”è®°æœ¬ç”µè„‘\">\n",
    "            <h2 class=\"product-name\">é«˜æ€§èƒ½ç¬”è®°æœ¬ç”µè„‘</h2>\n",
    "            <p class=\"price\" data-price=\"5999\">Â¥5,999</p>\n",
    "            <div class=\"rating\">\n",
    "                <span class=\"stars\">â˜…â˜…â˜…â˜…â˜…</span>\n",
    "                <span class=\"rating-score\">4.8</span>\n",
    "                <span class=\"review-count\">(238æ¡è¯„ä»·)</span>\n",
    "            </div>\n",
    "            <p class=\"description\">Intel i7å¤„ç†å™¨ï¼Œ16GBå†…å­˜ï¼Œ512GB SSD</p>\n",
    "            <div class=\"tags\">\n",
    "                <span class=\"tag\">çƒ­é”€</span>\n",
    "                <span class=\"tag\">åŒ…é‚®</span>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"product\" data-category=\"electronics\">\n",
    "            <img src=\"phone.jpg\" alt=\"æ™ºèƒ½æ‰‹æœº\">\n",
    "            <h2 class=\"product-name\">5Gæ™ºèƒ½æ‰‹æœº</h2>\n",
    "            <p class=\"price\" data-price=\"3999\">Â¥3,999</p>\n",
    "            <div class=\"rating\">\n",
    "                <span class=\"stars\">â˜…â˜…â˜…â˜…â˜†</span>\n",
    "                <span class=\"rating-score\">4.5</span>\n",
    "                <span class=\"review-count\">(156æ¡è¯„ä»·)</span>\n",
    "            </div>\n",
    "            <p class=\"description\">6.7è‹±å¯¸å±å¹•ï¼Œ128GBå­˜å‚¨ï¼Œä¸‰æ‘„åƒå¤´</p>\n",
    "            <div class=\"tags\">\n",
    "                <span class=\"tag\">æ–°å“</span>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"product\" data-category=\"books\">\n",
    "            <img src=\"python_book.jpg\" alt=\"Pythonç¼–ç¨‹ä¹¦\">\n",
    "            <h2 class=\"product-name\">Pythonç¼–ç¨‹ä»å…¥é—¨åˆ°å®è·µ</h2>\n",
    "            <p class=\"price\" data-price=\"89\">Â¥89</p>\n",
    "            <div class=\"rating\">\n",
    "                <span class=\"stars\">â˜…â˜…â˜…â˜…â˜…</span>\n",
    "                <span class=\"rating-score\">4.9</span>\n",
    "                <span class=\"review-count\">(892æ¡è¯„ä»·)</span>\n",
    "            </div>\n",
    "            <p class=\"description\">é›¶åŸºç¡€å­¦Pythonï¼Œå®æˆ˜é¡¹ç›®ä¸°å¯Œ</p>\n",
    "            <div class=\"tags\">\n",
    "                <span class=\"tag\">ç•…é”€</span>\n",
    "                <span class=\"tag\">åŒ…é‚®</span>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"product\" data-category=\"clothing\">\n",
    "            <img src=\"jacket.jpg\" alt=\"å†¬å­£å¤–å¥—\">\n",
    "            <h2 class=\"product-name\">ä¿æš–å†¬å­£å¤–å¥—</h2>\n",
    "            <p class=\"price\" data-price=\"299\">Â¥299</p>\n",
    "            <div class=\"rating\">\n",
    "                <span class=\"stars\">â˜…â˜…â˜…â˜…â˜†</span>\n",
    "                <span class=\"rating-score\">4.3</span>\n",
    "                <span class=\"review-count\">(67æ¡è¯„ä»·)</span>\n",
    "            </div>\n",
    "            <p class=\"description\">é˜²é£ä¿æš–ï¼Œå¤šè‰²å¯é€‰</p>\n",
    "            <div class=\"tags\">\n",
    "                <span class=\"tag\">é™æ—¶ä¼˜æƒ </span>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"statistics\">\n",
    "        <p>æ€»å•†å“æ•°ï¼š<span id=\"total-products\">1,250</span></p>\n",
    "        <p>ä»Šæ—¥é”€é‡ï¼š<span id=\"daily-sales\">89</span></p>\n",
    "        <p>ç”¨æˆ·è¯„åˆ†ï¼š<span id=\"avg-rating\">4.6</span></p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "print(\"ğŸ“„ è§£æHTMLå†…å®¹...\")\n",
    "\n",
    "# ä½¿ç”¨BeautifulSoupè§£æHTML\n",
    "soup = BeautifulSoup(sample_html, 'html.parser')\n",
    "\n",
    "# 1. æå–åŸºæœ¬ä¿¡æ¯\n",
    "print(\"\\nğŸª ç½‘ç«™åŸºæœ¬ä¿¡æ¯ï¼š\")\n",
    "title = soup.find('title').text\n",
    "print(f\"ç½‘ç«™æ ‡é¢˜ï¼š{title}\")\n",
    "\n",
    "# æå–å¯¼èˆªèœå•\n",
    "nav_links = soup.find('nav').find_all('a')\n",
    "print(\"å¯¼èˆªèœå•ï¼š\")\n",
    "for link in nav_links:\n",
    "    print(f\"  â€¢ {link.text} -> {link.get('href')}\")\n",
    "\n",
    "# 2. æå–äº§å“ä¿¡æ¯\n",
    "print(\"\\nğŸ›ï¸ äº§å“ä¿¡æ¯æå–ï¼š\")\n",
    "products = []\n",
    "\n",
    "for product_div in soup.find_all('div', class_='product'):\n",
    "    # æå–äº§å“åŸºæœ¬ä¿¡æ¯\n",
    "    name = product_div.find('h2', class_='product-name').text\n",
    "    price_text = product_div.find('p', class_='price').text\n",
    "    price_value = int(product_div.find('p', class_='price').get('data-price'))\n",
    "    category = product_div.get('data-category')\n",
    "    description = product_div.find('p', class_='description').text\n",
    "    \n",
    "    # æå–è¯„åˆ†ä¿¡æ¯\n",
    "    rating_div = product_div.find('div', class_='rating')\n",
    "    rating_score = float(rating_div.find('span', class_='rating-score').text)\n",
    "    review_text = rating_div.find('span', class_='review-count').text\n",
    "    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–è¯„ä»·æ•°é‡\n",
    "    review_count = int(re.search(r'(\\d+)', review_text).group(1))\n",
    "    \n",
    "    # æå–æ ‡ç­¾\n",
    "    tag_elements = product_div.find_all('span', class_='tag')\n",
    "    tags = [tag.text for tag in tag_elements]\n",
    "    \n",
    "    products.append({\n",
    "        'äº§å“åç§°': name,\n",
    "        'ä»·æ ¼': price_value,\n",
    "        'ä»·æ ¼æ˜¾ç¤º': price_text,\n",
    "        'ç±»åˆ«': category,\n",
    "        'æè¿°': description,\n",
    "        'è¯„åˆ†': rating_score,\n",
    "        'è¯„ä»·æ•°é‡': review_count,\n",
    "        'æ ‡ç­¾': ', '.join(tags)\n",
    "    })\n",
    "\n",
    "# è½¬æ¢ä¸ºDataFrameè¿›è¡Œæ•°æ®åˆ†æ\n",
    "scraped_df = pd.DataFrame(products)\n",
    "print(f\"æˆåŠŸæŠ“å– {len(scraped_df)} ä¸ªäº§å“\")\n",
    "print(\"\\näº§å“æ•°æ®é¢„è§ˆï¼š\")\n",
    "print(scraped_df)\n",
    "\n",
    "# 3. æ•°æ®æ¸…æ´—å’Œå¤„ç†\n",
    "print(\"\\nğŸ”§ æ•°æ®å¤„ç†å’Œåˆ†æï¼š\")\n",
    "\n",
    "# æŒ‰ç±»åˆ«åˆ†ç»„ç»Ÿè®¡\n",
    "category_stats = scraped_df.groupby('ç±»åˆ«').agg({\n",
    "    'ä»·æ ¼': ['mean', 'min', 'max', 'count'],\n",
    "    'è¯„åˆ†': 'mean',\n",
    "    'è¯„ä»·æ•°é‡': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "print(\"å„ç±»åˆ«ç»Ÿè®¡ï¼š\")\n",
    "print(category_stats)\n",
    "\n",
    "# 4. æ•°æ®å¯è§†åŒ–\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('ğŸ•·ï¸ ç½‘é¡µæŠ“å–æ•°æ®åˆ†æ', fontsize=16, fontweight='bold')\n",
    "\n",
    "# å­å›¾1ï¼šå„ç±»åˆ«ä»·æ ¼åˆ†å¸ƒ\n",
    "category_prices = scraped_df.groupby('ç±»åˆ«')['ä»·æ ¼'].mean()\n",
    "bars = axes[0,0].bar(category_prices.index, category_prices.values, \n",
    "                     color=['#FF6B6B', '#4ECDC4', '#45B7D1'], alpha=0.8)\n",
    "axes[0,0].set_title('ğŸ“Š å„ç±»åˆ«å¹³å‡ä»·æ ¼', fontsize=12, fontweight='bold')\n",
    "axes[0,0].set_ylabel('ä»·æ ¼ (å…ƒ)')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "for bar, value in zip(bars, category_prices.values):\n",
    "    axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50, \n",
    "                   f'Â¥{value:.0f}', ha='center', va='bottom')\n",
    "\n",
    "# å­å›¾2ï¼šè¯„åˆ†ä¸ä»·æ ¼å…³ç³»\n",
    "scatter = axes[0,1].scatter(scraped_df['ä»·æ ¼'], scraped_df['è¯„åˆ†'], \n",
    "                           s=scraped_df['è¯„ä»·æ•°é‡']*0.5, alpha=0.7, c=range(len(scraped_df)), cmap='viridis')\n",
    "axes[0,1].set_title('ğŸ’ ä»·æ ¼ä¸è¯„åˆ†å…³ç³»', fontsize=12, fontweight='bold')\n",
    "axes[0,1].set_xlabel('ä»·æ ¼ (å…ƒ)')\n",
    "axes[0,1].set_ylabel('è¯„åˆ†')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# å­å›¾3ï¼šè¯„ä»·æ•°é‡åˆ†å¸ƒ\n",
    "axes[1,0].bar(scraped_df['äº§å“åç§°'], scraped_df['è¯„ä»·æ•°é‡'], \n",
    "              color='#96CEB4', alpha=0.8)\n",
    "axes[1,0].set_title('ğŸ“ å„äº§å“è¯„ä»·æ•°é‡', fontsize=12, fontweight='bold')\n",
    "axes[1,0].set_ylabel('è¯„ä»·æ•°é‡')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# å­å›¾4ï¼šäº§å“æ ‡ç­¾è¯äº‘æ•ˆæœï¼ˆç”¨æŸ±çŠ¶å›¾æ¨¡æ‹Ÿï¼‰\n",
    "all_tags = []\n",
    "for tags in scraped_df['æ ‡ç­¾']:\n",
    "    all_tags.extend(tags.split(', '))\n",
    "tag_counts = pd.Series(all_tags).value_counts()\n",
    "\n",
    "axes[1,1].barh(tag_counts.index, tag_counts.values, color='#FFEAA7', alpha=0.8)\n",
    "axes[1,1].set_title('ğŸ·ï¸ æ ‡ç­¾ä½¿ç”¨é¢‘ç‡', fontsize=12, fontweight='bold')\n",
    "axes[1,1].set_xlabel('ä½¿ç”¨æ¬¡æ•°')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. é«˜çº§æ•°æ®æå–ç¤ºä¾‹\n",
    "print(\"\\nğŸ” é«˜çº§æ•°æ®æå–ç¤ºä¾‹ï¼š\")\n",
    "\n",
    "# æå–ç½‘ç«™ç»Ÿè®¡ä¿¡æ¯\n",
    "statistics_div = soup.find('div', class_='statistics')\n",
    "stats = {}\n",
    "for p in statistics_div.find_all('p'):\n",
    "    text = p.text\n",
    "    if 'æ€»å•†å“æ•°' in text:\n",
    "        stats['æ€»å•†å“æ•°'] = p.find('span').text\n",
    "    elif 'ä»Šæ—¥é”€é‡' in text:\n",
    "        stats['ä»Šæ—¥é”€é‡'] = p.find('span').text\n",
    "    elif 'ç”¨æˆ·è¯„åˆ†' in text:\n",
    "        stats['ç”¨æˆ·è¯„åˆ†'] = p.find('span').text\n",
    "\n",
    "print(\"ç½‘ç«™ç»Ÿè®¡ä¿¡æ¯ï¼š\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  â€¢ {key}: {value}\")\n",
    "\n",
    "# è®¡ç®—ä¸€äº›ä¸šåŠ¡æŒ‡æ ‡\n",
    "print(\"\\nğŸ“ˆ ä¸šåŠ¡æŒ‡æ ‡è®¡ç®—ï¼š\")\n",
    "avg_price = scraped_df['ä»·æ ¼'].mean()\n",
    "highest_rated = scraped_df.loc[scraped_df['è¯„åˆ†'].idxmax()]\n",
    "most_reviewed = scraped_df.loc[scraped_df['è¯„ä»·æ•°é‡'].idxmax()]\n",
    "\n",
    "print(f\"â€¢ å¹³å‡äº§å“ä»·æ ¼: Â¥{avg_price:.2f}\")\n",
    "print(f\"â€¢ è¯„åˆ†æœ€é«˜äº§å“: {highest_rated['äº§å“åç§°']} (è¯„åˆ†: {highest_rated['è¯„åˆ†']})\")\n",
    "print(f\"â€¢ è¯„ä»·æœ€å¤šäº§å“: {most_reviewed['äº§å“åç§°']} (è¯„ä»·: {most_reviewed['è¯„ä»·æ•°é‡']}æ¡)\")\n",
    "\n",
    "# ä»·æ ¼åŒºé—´åˆ†æ\n",
    "price_ranges = pd.cut(scraped_df['ä»·æ ¼'], bins=[0, 100, 1000, 5000, 10000], \n",
    "                     labels=['0-100å…ƒ', '100-1000å…ƒ', '1000-5000å…ƒ', '5000å…ƒä»¥ä¸Š'])\n",
    "price_distribution = price_ranges.value_counts()\n",
    "print(f\"\\nä»·æ ¼åŒºé—´åˆ†å¸ƒ:\")\n",
    "for range_name, count in price_distribution.items():\n",
    "    print(f\"  â€¢ {range_name}: {count}ä¸ªäº§å“\")\n",
    "\n",
    "print(\"\\nâœ… ç½‘é¡µæŠ“å–å®Œæˆï¼æˆåŠŸæå–å¹¶åˆ†æäº†æ‰€æœ‰äº§å“æ•°æ®ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a436ed8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. ğŸ¤– TensorFlow/Pytorch = æœºå™¨å­¦ä¹ æ¨¡å‹\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bd8785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error\n",
    "\n",
    "print(\"ğŸ”¹ ä½¿ç”¨PyTorchæ„å»ºæœºå™¨å­¦ä¹ æ¨¡å‹\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# æ£€æŸ¥PyTorchç‰ˆæœ¬å’ŒGPU\n",
    "print(f\"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"âœ… å¯ç”¨GPU: {'å¯ç”¨' if torch.cuda.is_available() else 'ä¸å¯ç”¨'}\")\n",
    "\n",
    "# 1. åˆ›å»ºæˆ¿ä»·é¢„æµ‹æ•°æ®é›†ï¼ˆå›å½’é—®é¢˜ï¼‰\n",
    "print(\"\\nğŸ  åˆ›å»ºæˆ¿ä»·é¢„æµ‹æ•°æ®é›†...\")\n",
    "np.random.seed(42)\n",
    "n_samples = 2000\n",
    "\n",
    "# ç”Ÿæˆç‰¹å¾æ•°æ®\n",
    "house_data = pd.DataFrame({\n",
    "    'é¢ç§¯': np.random.normal(120, 40, n_samples),  # å¹³æ–¹ç±³\n",
    "    'æˆ¿é—´æ•°': np.random.randint(1, 6, n_samples),   # æˆ¿é—´æ•°é‡\n",
    "    'æ¥¼å±‚': np.random.randint(1, 31, n_samples),    # æ¥¼å±‚\n",
    "    'æˆ¿é¾„': np.random.randint(0, 30, n_samples),    # æˆ¿é¾„ï¼ˆå¹´ï¼‰\n",
    "    'è·ç¦»åœ°é“': np.random.exponential(2, n_samples),  # è·ç¦»åœ°é“ç«™ï¼ˆå…¬é‡Œï¼‰\n",
    "    'å­¦åŒºæˆ¿': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),  # æ˜¯å¦å­¦åŒºæˆ¿\n",
    "    'è£…ä¿®æƒ…å†µ': np.random.choice([0, 1, 2], n_samples, p=[0.3, 0.5, 0.2])  # 0ç®€è£… 1ç²¾è£… 2è±ªè£…\n",
    "})\n",
    "\n",
    "# ç¡®ä¿æ•°æ®åˆç†æ€§\n",
    "house_data['é¢ç§¯'] = np.clip(house_data['é¢ç§¯'], 30, 300)\n",
    "house_data['è·ç¦»åœ°é“'] = np.clip(house_data['è·ç¦»åœ°é“'], 0.1, 10)\n",
    "\n",
    "# ç”Ÿæˆç›®æ ‡å˜é‡ï¼ˆæˆ¿ä»·ï¼‰- åŸºäºç‰¹å¾çš„å¤æ‚å…³ç³»\n",
    "base_price = (\n",
    "    house_data['é¢ç§¯'] * 500 +                    # é¢ç§¯å½±å“\n",
    "    house_data['æˆ¿é—´æ•°'] * 15000 +                # æˆ¿é—´æ•°å½±å“\n",
    "    (31 - house_data['æ¥¼å±‚']) * 2000 +            # æ¥¼å±‚å½±å“ï¼ˆé«˜æ¥¼å±‚æ›´è´µï¼‰\n",
    "    (30 - house_data['æˆ¿é¾„']) * 3000 +            # æˆ¿é¾„å½±å“ï¼ˆæ–°æˆ¿æ›´è´µï¼‰\n",
    "    (10 - house_data['è·ç¦»åœ°é“']) * 8000 +        # åœ°é“è·ç¦»å½±å“\n",
    "    house_data['å­¦åŒºæˆ¿'] * 200000 +               # å­¦åŒºæˆ¿åŠ æˆ\n",
    "    house_data['è£…ä¿®æƒ…å†µ'] * 50000                # è£…ä¿®æƒ…å†µå½±å“\n",
    ")\n",
    "\n",
    "# æ·»åŠ å™ªå£°\n",
    "noise = np.random.normal(0, 50000, n_samples)\n",
    "house_data['ä»·æ ¼'] = base_price + noise\n",
    "house_data['ä»·æ ¼'] = np.clip(house_data['ä»·æ ¼'], 100000, 2000000)  # é™åˆ¶ä»·æ ¼èŒƒå›´\n",
    "\n",
    "print(\"æ•°æ®é›†åŸºæœ¬ä¿¡æ¯ï¼š\")\n",
    "print(house_data.describe())\n",
    "print(f\"\\næ•°æ®é›†å½¢çŠ¶: {house_data.shape}\")\n",
    "\n",
    "# 2. æ•°æ®é¢„å¤„ç†\n",
    "print(\"\\nğŸ”§ æ•°æ®é¢„å¤„ç†...\")\n",
    "X = house_data.drop('ä»·æ ¼', axis=1).values\n",
    "y = house_data['ä»·æ ¼'].values.reshape(-1, 1)\n",
    "\n",
    "# æ•°æ®åˆ†å‰²\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ç‰¹å¾æ ‡å‡†åŒ–\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# è½¬æ¢ä¸ºPyTorchå¼ é‡\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# åˆ›å»ºéªŒè¯é›†\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}\")\n",
    "print(f\"éªŒè¯é›†å¤§å°: {len(val_dataset)}\")\n",
    "print(f\"æµ‹è¯•é›†å¤§å°: {len(test_dataset)}\")\n",
    "\n",
    "# 3. æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹\n",
    "print(\"\\nğŸ§  æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹...\")\n",
    "\n",
    "class HousePriceModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(HousePriceModel, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# åˆ›å»ºæ¨¡å‹\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HousePriceModel(X_train_scaled.shape[1]).to(device)\n",
    "print(\"æ¨¡å‹ç»“æ„ï¼š\")\n",
    "print(model)\n",
    "\n",
    "# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "print(\"\\nğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...\")\n",
    "num_epochs = 100\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "# å­˜å‚¨è®­ç»ƒå†å²\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_mae': [],\n",
    "    'val_mae': []\n",
    "}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # è®­ç»ƒé˜¶æ®µ\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_mae = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # å‰å‘ä¼ æ’­\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # åå‘ä¼ æ’­å’Œä¼˜åŒ–\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # è®¡ç®—ç»Ÿè®¡é‡\n",
    "        train_loss += loss.item()\n",
    "        train_mae += torch.mean(torch.abs(outputs - targets)).item()\n",
    "    \n",
    "    # éªŒè¯é˜¶æ®µ\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_mae = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            val_loss += criterion(outputs, targets).item()\n",
    "            val_mae += torch.mean(torch.abs(outputs - targets)).item()\n",
    "    \n",
    "    # è®¡ç®—å¹³å‡æŸå¤±\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    train_mae /= len(train_loader)\n",
    "    val_mae /= len(val_loader)\n",
    "    \n",
    "    # å­˜å‚¨å†å²\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_mae'].append(train_mae)\n",
    "    history['val_mae'].append(val_mae)\n",
    "    \n",
    "    # æ‰“å°è¿›åº¦\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "          f\"Train MAE: {train_mae:.2f}, Val MAE: {val_mae:.2f}\")\n",
    "    \n",
    "    # æ—©åœæœºåˆ¶\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "        # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"æ—©åœåœ¨ç¬¬ {epoch+1} è½®\")\n",
    "            break\n",
    "\n",
    "# åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# 4. æ¨¡å‹è¯„ä¼°\n",
    "print(\"\\nğŸ“Š æ¨¡å‹è¯„ä¼°...\")\n",
    "\n",
    "# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°\n",
    "test_loss = 0.0\n",
    "test_mae = 0.0\n",
    "all_targets = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        test_loss += criterion(outputs, targets).item()\n",
    "        test_mae += torch.mean(torch.abs(outputs - targets)).item()\n",
    "        \n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "        all_predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "# è®¡ç®—å¹³å‡æŸå¤±\n",
    "test_loss /= len(test_loader)\n",
    "test_mae /= len(test_loader)\n",
    "\n",
    "# åˆå¹¶é¢„æµ‹ç»“æœ\n",
    "all_targets = np.concatenate(all_targets)\n",
    "all_predictions = np.concatenate(all_predictions)\n",
    "\n",
    "# è®¡ç®—æ›´å¤šæŒ‡æ ‡\n",
    "r2 = r2_score(all_targets, all_predictions)\n",
    "mape = mean_absolute_percentage_error(all_targets, all_predictions)\n",
    "\n",
    "print(f\"æµ‹è¯•é›†æŸå¤± (MSE): {test_loss:.2f}\")\n",
    "print(f\"æµ‹è¯•é›†å¹³å‡ç»å¯¹è¯¯å·® (MAE): {test_mae:.2f}\")\n",
    "print(f\"RÂ² åˆ†æ•°: {r2:.4f}\")\n",
    "print(f\"å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·® (MAPE): {mape:.4f}\")\n",
    "\n",
    "# 5. å¯è§†åŒ–ç»“æœ\n",
    "print(\"\\nğŸ“ˆ ç»“æœå¯è§†åŒ–...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('ğŸ¤– PyTorchæˆ¿ä»·é¢„æµ‹æ¨¡å‹åˆ†æ', fontsize=16, fontweight='bold')\n",
    "\n",
    "# å­å›¾1ï¼šè®­ç»ƒå†å²\n",
    "axes[0,0].plot(history['train_loss'], label='è®­ç»ƒæŸå¤±', color='blue')\n",
    "axes[0,0].plot(history['val_loss'], label='éªŒè¯æŸå¤±', color='red')\n",
    "axes[0,0].set_title('ğŸ“‰ æ¨¡å‹è®­ç»ƒå†å²', fontsize=12, fontweight='bold')\n",
    "axes[0,0].set_xlabel('è½®æ¬¡')\n",
    "axes[0,0].set_ylabel('æŸå¤±')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# å­å›¾2ï¼šMAEå†å²\n",
    "axes[0,1].plot(history['train_mae'], label='è®­ç»ƒMAE', color='green')\n",
    "axes[0,1].plot(history['val_mae'], label='éªŒè¯MAE', color='orange')\n",
    "axes[0,1].set_title('ğŸ“Š å¹³å‡ç»å¯¹è¯¯å·®å†å²', fontsize=12, fontweight='bold')\n",
    "axes[0,1].set_xlabel('è½®æ¬¡')\n",
    "axes[0,1].set_ylabel('MAE')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# å­å›¾3ï¼šé¢„æµ‹vså®é™…\n",
    "axes[1,0].scatter(all_targets, all_predictions, alpha=0.6, color='purple')\n",
    "axes[1,0].plot([all_targets.min(), all_targets.max()], \n",
    "              [all_targets.min(), all_targets.max()], 'r--', lw=2)\n",
    "axes[1,0].set_title('ğŸ¯ é¢„æµ‹å€¼ vs å®é™…å€¼', fontsize=12, fontweight='bold')\n",
    "axes[1,0].set_xlabel('å®é™…æˆ¿ä»·')\n",
    "axes[1,0].set_ylabel('é¢„æµ‹æˆ¿ä»·')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# å­å›¾4ï¼šæ®‹å·®åˆ†å¸ƒ\n",
    "# residuals = all_targets - all_predictions.flatten()\n",
    "residuals = all_targets.flatten() - all_predictions.flatten()\n",
    "axes[1,1].hist(residuals, bins=30, alpha=0.7, color='cyan', edgecolor='black')\n",
    "axes[1,1].set_title('ğŸ“Š æ®‹å·®åˆ†å¸ƒ', fontsize=12, fontweight='bold')\n",
    "axes[1,1].set_xlabel('æ®‹å·®')\n",
    "axes[1,1].set_ylabel('é¢‘æ¬¡')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pytorch_house_price_analysis.png')\n",
    "plt.show()\n",
    "\n",
    "# 6. ç‰¹å¾é‡è¦æ€§åˆ†æ\n",
    "print(\"\\nğŸ” ç‰¹å¾é‡è¦æ€§åˆ†æ...\")\n",
    "\n",
    "# è·å–ç¬¬ä¸€å±‚æƒé‡\n",
    "first_layer = model.network[0]\n",
    "weights = first_layer.weight.data.cpu().numpy()\n",
    "feature_importance = np.abs(weights).mean(axis=0)\n",
    "\n",
    "feature_names = house_data.drop('ä»·æ ¼', axis=1).columns\n",
    "importance_df = pd.DataFrame({\n",
    "    'ç‰¹å¾': feature_names,\n",
    "    'é‡è¦æ€§': feature_importance\n",
    "}).sort_values('é‡è¦æ€§', ascending=False)\n",
    "\n",
    "print(\"ç‰¹å¾é‡è¦æ€§æ’åºï¼š\")\n",
    "print(importance_df)\n",
    "\n",
    "# ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(importance_df['ç‰¹å¾'], importance_df['é‡è¦æ€§'], \n",
    "               color='lightblue', alpha=0.8, edgecolor='navy')\n",
    "plt.title('ğŸ¯ ç‰¹å¾é‡è¦æ€§åˆ†æ', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('ç‰¹å¾')\n",
    "plt.ylabel('é‡è¦æ€§åˆ†æ•°')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for bar, value in zip(bars, importance_df['é‡è¦æ€§']):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "            f'{value:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png')\n",
    "plt.show()\n",
    "\n",
    "# 7. å®é™…é¢„æµ‹ç¤ºä¾‹\n",
    "print(\"\\nğŸ¡ å®é™…é¢„æµ‹ç¤ºä¾‹...\")\n",
    "\n",
    "# åˆ›å»ºå‡ ä¸ªç¤ºä¾‹æˆ¿å­\n",
    "sample_houses = pd.DataFrame({\n",
    "    'é¢ç§¯': [90, 130, 200],\n",
    "    'æˆ¿é—´æ•°': [2, 3, 4],\n",
    "    'æ¥¼å±‚': [15, 8, 3],\n",
    "    'æˆ¿é¾„': [5, 15, 2],\n",
    "    'è·ç¦»åœ°é“': [0.8, 2.5, 1.2],\n",
    "    'å­¦åŒºæˆ¿': [1, 0, 1],\n",
    "    'è£…ä¿®æƒ…å†µ': [2, 1, 2]\n",
    "})\n",
    "\n",
    "# æ ‡å‡†åŒ–\n",
    "sample_scaled = scaler.transform(sample_houses)\n",
    "sample_tensor = torch.tensor(sample_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "# é¢„æµ‹\n",
    "with torch.no_grad():\n",
    "    predictions = model(sample_tensor).cpu().numpy()\n",
    "\n",
    "print(\"ç¤ºä¾‹æˆ¿å­é¢„æµ‹ç»“æœï¼š\")\n",
    "for i, (_, house) in enumerate(sample_houses.iterrows()):\n",
    "    pred_price = predictions[i][0]\n",
    "    print(f\"\\næˆ¿å­ {i+1}:\")\n",
    "    print(f\"  â€¢ é¢ç§¯: {house['é¢ç§¯']}ã¡\")\n",
    "    print(f\"  â€¢ æˆ¿é—´æ•°: {house['æˆ¿é—´æ•°']}å®¤\")\n",
    "    print(f\"  â€¢ æ¥¼å±‚: {house['æ¥¼å±‚']}å±‚\")\n",
    "    print(f\"  â€¢ æˆ¿é¾„: {house['æˆ¿é¾„']}å¹´\")\n",
    "    print(f\"  â€¢ è·ç¦»åœ°é“: {house['è·ç¦»åœ°é“']:.1f}km\")\n",
    "    print(f\"  â€¢ å­¦åŒºæˆ¿: {'æ˜¯' if house['å­¦åŒºæˆ¿'] else 'å¦'}\")\n",
    "    print(f\"  â€¢ è£…ä¿®: {['ç®€è£…', 'ç²¾è£…', 'è±ªè£…'][int(house['è£…ä¿®æƒ…å†µ'])]}\")\n",
    "    print(f\"  ğŸ·ï¸ é¢„æµ‹ä»·æ ¼: Â¥{pred_price:,.0f}\")\n",
    "\n",
    "print(\"\\nâœ… PyTorchæœºå™¨å­¦ä¹ ç¤ºä¾‹å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3934dc26",
   "metadata": {},
   "source": [
    "## 5. ğŸ–¼ï¸ OpenCV = è®¡ç®—æœºè§†è§‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2f0c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”¹ ä½¿ç”¨TensorFlowæ„å»ºæœºå™¨å­¦ä¹ æ¨¡å‹\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# æ£€æŸ¥å¹¶å®‰è£…ä¾èµ–\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    print(f\"âœ… TensorFlow ç‰ˆæœ¬: {tf.__version__}\")\n",
    "    print(f\"âœ… å¯ç”¨GPU: {len(tf.config.list_physical_devices('GPU'))} ä¸ª\")\n",
    "except ImportError:\n",
    "    print(\"âŒ è¯·å…ˆå®‰è£… TensorFlow: pip install tensorflow scikit-learn\")\n",
    "    print(\"ç»§ç»­å±•ç¤ºä»£ç ç¤ºä¾‹...\")\n",
    "\n",
    "# 1. åˆ›å»ºæˆ¿ä»·é¢„æµ‹æ•°æ®é›†ï¼ˆå›å½’é—®é¢˜ï¼‰\n",
    "print(\"\\nğŸ  åˆ›å»ºæˆ¿ä»·é¢„æµ‹æ•°æ®é›†...\")\n",
    "np.random.seed(42)\n",
    "n_samples = 2000\n",
    "\n",
    "# ç”Ÿæˆç‰¹å¾æ•°æ®\n",
    "house_data = pd.DataFrame({\n",
    "    'é¢ç§¯': np.random.normal(120, 40, n_samples),  # å¹³æ–¹ç±³\n",
    "    'æˆ¿é—´æ•°': np.random.randint(1, 6, n_samples),   # æˆ¿é—´æ•°é‡\n",
    "    'æ¥¼å±‚': np.random.randint(1, 31, n_samples),    # æ¥¼å±‚\n",
    "    'æˆ¿é¾„': np.random.randint(0, 30, n_samples),    # æˆ¿é¾„ï¼ˆå¹´ï¼‰\n",
    "    'è·ç¦»åœ°é“': np.random.exponential(2, n_samples),  # è·ç¦»åœ°é“ç«™ï¼ˆå…¬é‡Œï¼‰\n",
    "    'å­¦åŒºæˆ¿': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),  # æ˜¯å¦å­¦åŒºæˆ¿\n",
    "    'è£…ä¿®æƒ…å†µ': np.random.choice([0, 1, 2], n_samples, p=[0.3, 0.5, 0.2])  # 0ç®€è£… 1ç²¾è£… 2è±ªè£…\n",
    "})\n",
    "\n",
    "# ç¡®ä¿æ•°æ®åˆç†æ€§\n",
    "house_data['é¢ç§¯'] = np.clip(house_data['é¢ç§¯'], 30, 300)\n",
    "house_data['è·ç¦»åœ°é“'] = np.clip(house_data['è·ç¦»åœ°é“'], 0.1, 10)\n",
    "\n",
    "# ç”Ÿæˆç›®æ ‡å˜é‡ï¼ˆæˆ¿ä»·ï¼‰- åŸºäºç‰¹å¾çš„å¤æ‚å…³ç³»\n",
    "base_price = (\n",
    "    house_data['é¢ç§¯'] * 500 +                    # é¢ç§¯å½±å“\n",
    "    house_data['æˆ¿é—´æ•°'] * 15000 +                # æˆ¿é—´æ•°å½±å“\n",
    "    (31 - house_data['æ¥¼å±‚']) * 2000 +            # æ¥¼å±‚å½±å“ï¼ˆé«˜æ¥¼å±‚æ›´è´µï¼‰\n",
    "    (30 - house_data['æˆ¿é¾„']) * 3000 +            # æˆ¿é¾„å½±å“ï¼ˆæ–°æˆ¿æ›´è´µï¼‰\n",
    "    (10 - house_data['è·ç¦»åœ°é“']) * 8000 +        # åœ°é“è·ç¦»å½±å“\n",
    "    house_data['å­¦åŒºæˆ¿'] * 200000 +               # å­¦åŒºæˆ¿åŠ æˆ\n",
    "    house_data['è£…ä¿®æƒ…å†µ'] * 50000                # è£…ä¿®æƒ…å†µå½±å“\n",
    ")\n",
    "\n",
    "# æ·»åŠ å™ªå£°\n",
    "noise = np.random.normal(0, 50000, n_samples)\n",
    "house_data['ä»·æ ¼'] = base_price + noise\n",
    "house_data['ä»·æ ¼'] = np.clip(house_data['ä»·æ ¼'], 100000, 2000000)  # é™åˆ¶ä»·æ ¼èŒƒå›´\n",
    "\n",
    "print(\"æ•°æ®é›†åŸºæœ¬ä¿¡æ¯ï¼š\")\n",
    "print(house_data.describe())\n",
    "print(f\"\\næ•°æ®é›†å½¢çŠ¶: {house_data.shape}\")\n",
    "\n",
    "# 2. æ•°æ®é¢„å¤„ç†\n",
    "print(\"\\nğŸ”§ æ•°æ®é¢„å¤„ç†...\")\n",
    "X = house_data.drop('ä»·æ ¼', axis=1)\n",
    "y = house_data['ä»·æ ¼']\n",
    "\n",
    "# æ•°æ®åˆ†å‰²\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ç‰¹å¾æ ‡å‡†åŒ–\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"è®­ç»ƒé›†å¤§å°: {X_train_scaled.shape}\")\n",
    "print(f\"æµ‹è¯•é›†å¤§å°: {X_test_scaled.shape}\")\n",
    "\n",
    "# 3. æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹\n",
    "print(\"\\nğŸ§  æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹...\")\n",
    "\n",
    "try:\n",
    "    # åˆ›å»ºå›å½’æ¨¡å‹\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)  # è¾“å‡ºå±‚ï¼Œå›å½’é—®é¢˜\n",
    "    ])\n",
    "\n",
    "    # ç¼–è¯‘æ¨¡å‹\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "    # æ˜¾ç¤ºæ¨¡å‹ç»“æ„\n",
    "    print(\"æ¨¡å‹ç»“æ„ï¼š\")\n",
    "    model.summary()\n",
    "\n",
    "    # è®­ç»ƒæ¨¡å‹\n",
    "    print(\"\\nğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...\")\n",
    "    \n",
    "    # è®¾ç½®å›è°ƒå‡½æ•°\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=10, restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001\n",
    "    )\n",
    "\n",
    "    # è®­ç»ƒæ¨¡å‹\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # 4. æ¨¡å‹è¯„ä¼°\n",
    "    print(\"\\nğŸ“Š æ¨¡å‹è¯„ä¼°...\")\n",
    "    \n",
    "    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°\n",
    "    test_loss, test_mae = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "    print(f\"æµ‹è¯•é›†æŸå¤± (MSE): {test_loss:.2f}\")\n",
    "    print(f\"æµ‹è¯•é›†å¹³å‡ç»å¯¹è¯¯å·® (MAE): {test_mae:.2f}\")\n",
    "\n",
    "    # é¢„æµ‹\n",
    "    y_pred = model.predict(X_test_scaled, verbose=0)\n",
    "    \n",
    "    # è®¡ç®—æ›´å¤šæŒ‡æ ‡\n",
    "    from sklearn.metrics import r2_score, mean_absolute_percentage_error\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    \n",
    "    print(f\"RÂ² åˆ†æ•°: {r2:.4f}\")\n",
    "    print(f\"å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·® (MAPE): {mape:.4f}\")\n",
    "\n",
    "    # 5. å¯è§†åŒ–ç»“æœ\n",
    "    print(\"\\nğŸ“ˆ ç»“æœå¯è§†åŒ–...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('ğŸ¤– TensorFlowæˆ¿ä»·é¢„æµ‹æ¨¡å‹åˆ†æ', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # å­å›¾1ï¼šè®­ç»ƒå†å²\n",
    "    axes[0,0].plot(history.history['loss'], label='è®­ç»ƒæŸå¤±', color='blue')\n",
    "    axes[0,0].plot(history.history['val_loss'], label='éªŒè¯æŸå¤±', color='red')\n",
    "    axes[0,0].set_title('ğŸ“‰ æ¨¡å‹è®­ç»ƒå†å²', fontsize=12, fontweight='bold')\n",
    "    axes[0,0].set_xlabel('è½®æ¬¡')\n",
    "    axes[0,0].set_ylabel('æŸå¤±')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "    # å­å›¾2ï¼šMAEå†å²\n",
    "    axes[0,1].plot(history.history['mae'], label='è®­ç»ƒMAE', color='green')\n",
    "    axes[0,1].plot(history.history['val_mae'], label='éªŒè¯MAE', color='orange')\n",
    "    axes[0,1].set_title('ğŸ“Š å¹³å‡ç»å¯¹è¯¯å·®å†å²', fontsize=12, fontweight='bold')\n",
    "    axes[0,1].set_xlabel('è½®æ¬¡')\n",
    "    axes[0,1].set_ylabel('MAE')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "    # å­å›¾3ï¼šé¢„æµ‹vså®é™…\n",
    "    axes[1,0].scatter(y_test, y_pred, alpha=0.6, color='purple')\n",
    "    axes[1,0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    axes[1,0].set_title('ğŸ¯ é¢„æµ‹å€¼ vs å®é™…å€¼', fontsize=12, fontweight='bold')\n",
    "    axes[1,0].set_xlabel('å®é™…æˆ¿ä»·')\n",
    "    axes[1,0].set_ylabel('é¢„æµ‹æˆ¿ä»·')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "    # å­å›¾4ï¼šæ®‹å·®åˆ†å¸ƒ\n",
    "    residuals = y_test - y_pred.flatten()\n",
    "    axes[1,1].hist(residuals, bins=30, alpha=0.7, color='cyan', edgecolor='black')\n",
    "    axes[1,1].set_title('ğŸ“Š æ®‹å·®åˆ†å¸ƒ', fontsize=12, fontweight='bold')\n",
    "    axes[1,1].set_xlabel('æ®‹å·®')\n",
    "    axes[1,1].set_ylabel('é¢‘æ¬¡')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 6. ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆä½¿ç”¨æƒé‡è¿‘ä¼¼ï¼‰\n",
    "    print(\"\\nğŸ” ç‰¹å¾é‡è¦æ€§åˆ†æ...\")\n",
    "    \n",
    "    # è·å–ç¬¬ä¸€å±‚æƒé‡çš„ç»å¯¹å€¼å¹³å‡ä½œä¸ºç‰¹å¾é‡è¦æ€§æŒ‡æ ‡\n",
    "    first_layer_weights = model.layers[0].get_weights()[0]\n",
    "    feature_importance = np.abs(first_layer_weights).mean(axis=1)\n",
    "    \n",
    "    feature_names = X.columns\n",
    "    importance_df = pd.DataFrame({\n",
    "        'ç‰¹å¾': feature_names,\n",
    "        'é‡è¦æ€§': feature_importance\n",
    "    }).sort_values('é‡è¦æ€§', ascending=False)\n",
    "    \n",
    "    print(\"ç‰¹å¾é‡è¦æ€§æ’åºï¼š\")\n",
    "    print(importance_df)\n",
    "    \n",
    "    # ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(importance_df['ç‰¹å¾'], importance_df['é‡è¦æ€§'], \n",
    "                   color='lightblue', alpha=0.8, edgecolor='navy')\n",
    "    plt.title('ğŸ¯ ç‰¹å¾é‡è¦æ€§åˆ†æ', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('ç‰¹å¾')\n",
    "    plt.ylabel('é‡è¦æ€§åˆ†æ•°')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "    for bar, value in zip(bars, importance_df['é‡è¦æ€§']):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "                f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 7. å®é™…é¢„æµ‹ç¤ºä¾‹\n",
    "    print(\"\\nğŸ¡ å®é™…é¢„æµ‹ç¤ºä¾‹...\")\n",
    "    \n",
    "    # åˆ›å»ºå‡ ä¸ªç¤ºä¾‹æˆ¿å­\n",
    "    sample_houses = pd.DataFrame({\n",
    "        'é¢ç§¯': [90, 130, 200],\n",
    "        'æˆ¿é—´æ•°': [2, 3, 4],\n",
    "        'æ¥¼å±‚': [15, 8, 3],\n",
    "        'æˆ¿é¾„': [5, 15, 2],\n",
    "        'è·ç¦»åœ°é“': [0.8, 2.5, 1.2],\n",
    "        'å­¦åŒºæˆ¿': [1, 0, 1],\n",
    "        'è£…ä¿®æƒ…å†µ': [2, 1, 2]\n",
    "    })\n",
    "    \n",
    "    # æ ‡å‡†åŒ–\n",
    "    sample_scaled = scaler.transform(sample_houses)\n",
    "    \n",
    "    # é¢„æµ‹\n",
    "    predictions = model.predict(sample_scaled, verbose=0)\n",
    "    \n",
    "    print(\"ç¤ºä¾‹æˆ¿å­é¢„æµ‹ç»“æœï¼š\")\n",
    "    for i, (_, house) in enumerate(sample_houses.iterrows()):\n",
    "        pred_price = predictions[i][0]\n",
    "        print(f\"\\næˆ¿å­ {i+1}:\")\n",
    "        print(f\"  â€¢ é¢ç§¯: {house['é¢ç§¯']}ã¡\")\n",
    "        print(f\"  â€¢ æˆ¿é—´æ•°: {house['æˆ¿é—´æ•°']}å®¤\")\n",
    "        print(f\"  â€¢ æ¥¼å±‚: {house['æ¥¼å±‚']}å±‚\")\n",
    "        print(f\"  â€¢ æˆ¿é¾„: {house['æˆ¿é¾„']}å¹´\")\n",
    "        print(f\"  â€¢ è·ç¦»åœ°é“: {house['è·ç¦»åœ°é“']:.1f}km\")\n",
    "        print(f\"  â€¢ å­¦åŒºæˆ¿: {'æ˜¯' if house['å­¦åŒºæˆ¿'] else 'å¦'}\")\n",
    "        print(f\"  â€¢ è£…ä¿®: {['ç®€è£…', 'ç²¾è£…', 'è±ªè£…'][house['è£…ä¿®æƒ…å†µ']]}\")\n",
    "        print(f\"  ğŸ·ï¸ é¢„æµ‹ä»·æ ¼: Â¥{pred_price:,.0f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ¨¡å‹è®­ç»ƒå‡ºé”™: {e}\")\n",
    "    print(\"è¿™å¯èƒ½æ˜¯å› ä¸ºæ²¡æœ‰å®‰è£…TensorFlowæˆ–ç‰ˆæœ¬ä¸å…¼å®¹\")\n",
    "\n",
    "print(\"\\nâœ… TensorFlowæœºå™¨å­¦ä¹ ç¤ºä¾‹å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab09c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”¹ ä½¿ç”¨OpenCVè¿›è¡Œè®¡ç®—æœºè§†è§‰å¤„ç†\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# æ£€æŸ¥å¹¶å®‰è£…ä¾èµ–\n",
    "try:\n",
    "    import cv2\n",
    "    print(f\"âœ… OpenCV ç‰ˆæœ¬: {cv2.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"âŒ è¯·å…ˆå®‰è£… OpenCV: pip install opencv-python\")\n",
    "    print(\"ç»§ç»­å±•ç¤ºä»£ç ç¤ºä¾‹...\")\n",
    "\n",
    "# 1. åˆ›å»ºç¤ºä¾‹å›¾åƒ\n",
    "print(\"\\nğŸ¨ åˆ›å»ºå’Œå¤„ç†å›¾åƒ...\")\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªå½©è‰²ç”»å¸ƒ\n",
    "img_width, img_height = 800, 600\n",
    "canvas = np.zeros((img_height, img_width, 3), dtype=np.uint8)\n",
    "\n",
    "# è®¾ç½®èƒŒæ™¯æ¸å˜è‰²\n",
    "for i in range(img_height):\n",
    "    gradient_value = int(255 * (i / img_height))\n",
    "    canvas[i, :] = [gradient_value // 3, gradient_value // 2, gradient_value]\n",
    "\n",
    "print(\"ğŸ–Œï¸ åœ¨ç”»å¸ƒä¸Šç»˜åˆ¶å„ç§å›¾å½¢...\")\n",
    "\n",
    "try:\n",
    "    # ç»˜åˆ¶å‡ ä½•å›¾å½¢\n",
    "    # çŸ©å½¢\n",
    "    cv2.rectangle(canvas, (50, 50), (250, 200), (0, 255, 100), -1)  # å¡«å……ç»¿è‰²çŸ©å½¢\n",
    "    cv2.rectangle(canvas, (50, 50), (250, 200), (255, 255, 255), 3)  # ç™½è‰²è¾¹æ¡†\n",
    "    \n",
    "    # åœ†å½¢\n",
    "    cv2.circle(canvas, (400, 125), 80, (255, 100, 0), -1)  # å¡«å……æ©™è‰²åœ†å½¢\n",
    "    cv2.circle(canvas, (400, 125), 80, (255, 255, 255), 3)  # ç™½è‰²è¾¹æ¡†\n",
    "    \n",
    "    # æ¤­åœ†\n",
    "    cv2.ellipse(canvas, (600, 125), (100, 60), 45, 0, 360, (255, 0, 255), -1)  # ç´«è‰²æ¤­åœ†\n",
    "    cv2.ellipse(canvas, (600, 125), (100, 60), 45, 0, 360, (255, 255, 255), 3)\n",
    "    \n",
    "    # å¤šè¾¹å½¢\n",
    "    points = np.array([[150, 300], [50, 450], [100, 550], [200, 550], [250, 450]], np.int32)\n",
    "    cv2.fillPoly(canvas, [points], (0, 200, 255))  # å¡«å……é»„è‰²äº”è¾¹å½¢\n",
    "    cv2.polylines(canvas, [points], True, (255, 255, 255), 3)\n",
    "    \n",
    "    # ç»˜åˆ¶æ–‡å­—\n",
    "    cv2.putText(canvas, 'OpenCV Demo', (350, 350), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 3)\n",
    "    cv2.putText(canvas, 'Computer Vision', (320, 400), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (200, 200, 200), 2)\n",
    "    \n",
    "    # ç»˜åˆ¶ç›´çº¿\n",
    "    cv2.line(canvas, (50, 500), (750, 500), (255, 255, 0), 5)  # é»„è‰²æ°´å¹³çº¿\n",
    "    cv2.line(canvas, (400, 50), (400, 550), (0, 255, 255), 3)  # é’è‰²å‚ç›´çº¿\n",
    "    \n",
    "    print(\"âœ… å›¾å½¢ç»˜åˆ¶å®Œæˆ\")\n",
    "\n",
    "    # 2. å›¾åƒå¤„ç†æ“ä½œ\n",
    "    print(\"\\nğŸ”§ åº”ç”¨å„ç§å›¾åƒå¤„ç†æŠ€æœ¯...\")\n",
    "    \n",
    "    # é«˜æ–¯æ¨¡ç³Š\n",
    "    blurred = cv2.GaussianBlur(canvas, (15, 15), 0)\n",
    "    \n",
    "    # è½¬æ¢ä¸ºç°åº¦å›¾\n",
    "    gray = cv2.cvtColor(canvas, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # è¾¹ç¼˜æ£€æµ‹\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    \n",
    "    # å½¢æ€å­¦æ“ä½œ\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
    "    morphed = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    # HSVé¢œè‰²ç©ºé—´è½¬æ¢\n",
    "    hsv = cv2.cvtColor(canvas, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # é¢œè‰²èŒƒå›´æå–ï¼ˆæå–ç»¿è‰²åŒºåŸŸï¼‰\n",
    "    lower_green = np.array([40, 50, 50])\n",
    "    upper_green = np.array([80, 255, 255])\n",
    "    green_mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "    green_result = cv2.bitwise_and(canvas, canvas, mask=green_mask)\n",
    "    \n",
    "    # 3. åˆ›å»ºç»¼åˆå±•ç¤º\n",
    "    print(\"\\nğŸ“Š åˆ›å»ºå›¾åƒå¤„ç†ç»“æœå±•ç¤º...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "    fig.suptitle('ğŸ–¼ï¸ OpenCVè®¡ç®—æœºè§†è§‰å¤„ç†å±•ç¤º', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # åŸå§‹å›¾åƒ\n",
    "    axes[0,0].imshow(cv2.cvtColor(canvas, cv2.COLOR_BGR2RGB))\n",
    "    axes[0,0].set_title('ğŸ¨ åŸå§‹å›¾åƒ', fontsize=12, fontweight='bold')\n",
    "    axes[0,0].axis('off')\n",
    "    \n",
    "    # é«˜æ–¯æ¨¡ç³Š\n",
    "    axes[0,1].imshow(cv2.cvtColor(blurred, cv2.COLOR_BGR2RGB))\n",
    "    axes[0,1].set_title('ğŸŒ«ï¸ é«˜æ–¯æ¨¡ç³Š', fontsize=12, fontweight='bold')\n",
    "    axes[0,1].axis('off')\n",
    "    \n",
    "    # ç°åº¦å›¾åƒ\n",
    "    axes[0,2].imshow(gray, cmap='gray')\n",
    "    axes[0,2].set_title('âš« ç°åº¦å›¾åƒ', fontsize=12, fontweight='bold')\n",
    "    axes[0,2].axis('off')\n",
    "    \n",
    "    # è¾¹ç¼˜æ£€æµ‹\n",
    "    axes[1,0].imshow(edges, cmap='gray')\n",
    "    axes[1,0].set_title('ğŸ” Cannyè¾¹ç¼˜æ£€æµ‹', fontsize=12, fontweight='bold')\n",
    "    axes[1,0].axis('off')\n",
    "    \n",
    "    # å½¢æ€å­¦å¤„ç†\n",
    "    axes[1,1].imshow(morphed, cmap='gray')\n",
    "    axes[1,1].set_title('ğŸ”§ å½¢æ€å­¦å¤„ç†', fontsize=12, fontweight='bold')\n",
    "    axes[1,1].axis('off')\n",
    "    \n",
    "    # HSVé¢œè‰²ç©ºé—´\n",
    "    axes[1,2].imshow(cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB))\n",
    "    axes[1,2].set_title('ğŸŒˆ HSVé¢œè‰²ç©ºé—´', fontsize=12, fontweight='bold')\n",
    "    axes[1,2].axis('off')\n",
    "    \n",
    "    # é¢œè‰²æ©ç \n",
    "    axes[2,0].imshow(green_mask, cmap='gray')\n",
    "    axes[2,0].set_title('ğŸŸ¢ ç»¿è‰²åŒºåŸŸæ©ç ', fontsize=12, fontweight='bold')\n",
    "    axes[2,0].axis('off')\n",
    "    \n",
    "    # é¢œè‰²æå–ç»“æœ\n",
    "    axes[2,1].imshow(cv2.cvtColor(green_result, cv2.COLOR_BGR2RGB))\n",
    "    axes[2,1].set_title('ğŸ¯ ç»¿è‰²åŒºåŸŸæå–', fontsize=12, fontweight='bold')\n",
    "    axes[2,1].axis('off')\n",
    "    \n",
    "    # è½®å»“æ£€æµ‹\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contour_img = canvas.copy()\n",
    "    cv2.drawContours(contour_img, contours, -1, (255, 255, 0), 2)\n",
    "    axes[2,2].imshow(cv2.cvtColor(contour_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[2,2].set_title('ğŸ“ è½®å»“æ£€æµ‹', fontsize=12, fontweight='bold')\n",
    "    axes[2,2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. å›¾åƒåˆ†æå’Œç»Ÿè®¡\n",
    "    print(\"\\nğŸ“ˆ å›¾åƒåˆ†æå’Œç»Ÿè®¡...\")\n",
    "    \n",
    "    # è®¡ç®—é¢œè‰²ç›´æ–¹å›¾\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('ğŸ“Š å›¾åƒåˆ†æç»Ÿè®¡', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # RGBé¢œè‰²ç›´æ–¹å›¾\n",
    "    colors = ['red', 'green', 'blue']\n",
    "    for i, color in enumerate(colors):\n",
    "        hist = cv2.calcHist([canvas], [i], None, [256], [0, 256])\n",
    "        axes[0,0].plot(hist, color=color, alpha=0.7, linewidth=2)\n",
    "    axes[0,0].set_title('ğŸ¨ RGBé¢œè‰²ç›´æ–¹å›¾', fontsize=12, fontweight='bold')\n",
    "    axes[0,0].set_xlabel('åƒç´ å€¼')\n",
    "    axes[0,0].set_ylabel('é¢‘æ¬¡')\n",
    "    axes[0,0].legend(['Red', 'Green', 'Blue'])\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ç°åº¦ç›´æ–¹å›¾\n",
    "    gray_hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n",
    "    axes[0,1].plot(gray_hist, color='black', linewidth=2)\n",
    "    axes[0,1].fill_between(range(256), gray_hist.flatten(), alpha=0.3, color='gray')\n",
    "    axes[0,1].set_title('âš« ç°åº¦ç›´æ–¹å›¾', fontsize=12, fontweight='bold')\n",
    "    axes[0,1].set_xlabel('ç°åº¦å€¼')\n",
    "    axes[0,1].set_ylabel('é¢‘æ¬¡')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # è½®å»“åˆ†æ\n",
    "    contour_areas = [cv2.contourArea(contour) for contour in contours if cv2.contourArea(contour) > 100]\n",
    "    contour_perimeters = [cv2.arcLength(contour, True) for contour in contours if cv2.contourArea(contour) > 100]\n",
    "    \n",
    "    axes[1,0].bar(range(len(contour_areas)), contour_areas, color='skyblue', alpha=0.8)\n",
    "    axes[1,0].set_title('ğŸ“ è½®å»“é¢ç§¯åˆ†æ', fontsize=12, fontweight='bold')\n",
    "    axes[1,0].set_xlabel('è½®å»“ç¼–å·')\n",
    "    axes[1,0].set_ylabel('é¢ç§¯ (åƒç´ Â²)')\n",
    "    axes[1,0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # å›¾åƒå±æ€§ç»Ÿè®¡\n",
    "    stats_data = {\n",
    "        'å±æ€§': ['å›¾åƒå®½åº¦', 'å›¾åƒé«˜åº¦', 'æ€»åƒç´ æ•°', 'è½®å»“æ•°é‡', 'å¹³å‡äº®åº¦', 'æ ‡å‡†å·®'],\n",
    "        'æ•°å€¼': [\n",
    "            canvas.shape[1], \n",
    "            canvas.shape[0], \n",
    "            canvas.shape[0] * canvas.shape[1],\n",
    "            len(contours),\n",
    "            np.mean(gray),\n",
    "            np.std(gray)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    stats_df = pd.DataFrame(stats_data)\n",
    "    axes[1,1].axis('tight')\n",
    "    axes[1,1].axis('off')\n",
    "    table = axes[1,1].table(cellText=[[f'{val:.2f}' if isinstance(val, float) else str(val) for val in stats_df['æ•°å€¼']]], \n",
    "                           rowLabels=stats_df['å±æ€§'], \n",
    "                           colLabels=['æ•°å€¼'],\n",
    "                           cellLoc='center',\n",
    "                           loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.5)\n",
    "    axes[1,1].set_title('ğŸ“Š å›¾åƒå±æ€§ç»Ÿè®¡', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 5. å›¾åƒæ»¤æ³¢å™¨æ¼”ç¤º\n",
    "    print(\"\\nğŸ”¬ å„ç§å›¾åƒæ»¤æ³¢å™¨æ¼”ç¤º...\")\n",
    "    \n",
    "    # åº”ç”¨ä¸åŒçš„æ»¤æ³¢å™¨\n",
    "    # å‡å€¼æ»¤æ³¢\n",
    "    mean_filtered = cv2.blur(canvas, (15, 15))\n",
    "    \n",
    "    # ä¸­å€¼æ»¤æ³¢\n",
    "    median_filtered = cv2.medianBlur(canvas, 15)\n",
    "    \n",
    "    # åŒè¾¹æ»¤æ³¢\n",
    "    bilateral_filtered = cv2.bilateralFilter(canvas, 15, 80, 80)\n",
    "    \n",
    "    # æ‹‰æ™®æ‹‰æ–¯æ»¤æ³¢\n",
    "    laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n",
    "    laplacian = np.uint8(np.absolute(laplacian))\n",
    "    \n",
    "    # Sobelæ»¤æ³¢\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    sobel_combined = np.sqrt(sobelx**2 + sobely**2)\n",
    "    sobel_combined = np.uint8(sobel_combined)\n",
    "    \n",
    "    # å±•ç¤ºæ»¤æ³¢ç»“æœ\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('ğŸ”¬ å›¾åƒæ»¤æ³¢å™¨æ•ˆæœå±•ç¤º', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # åŸå›¾\n",
    "    axes[0,0].imshow(cv2.cvtColor(canvas, cv2.COLOR_BGR2RGB))\n",
    "    axes[0,0].set_title('ğŸ¨ åŸå§‹å›¾åƒ', fontsize=12, fontweight='bold')\n",
    "    axes[0,0].axis('off')\n",
    "    \n",
    "    # å‡å€¼æ»¤æ³¢\n",
    "    axes[0,1].imshow(cv2.cvtColor(mean_filtered, cv2.COLOR_BGR2RGB))\n",
    "    axes[0,1].set_title('ğŸ“Š å‡å€¼æ»¤æ³¢', fontsize=12, fontweight='bold')\n",
    "    axes[0,1].axis('off')\n",
    "    \n",
    "    # ä¸­å€¼æ»¤æ³¢\n",
    "    axes[0,2].imshow(cv2.cvtColor(median_filtered, cv2.COLOR_BGR2RGB))\n",
    "    axes[0,2].set_title('ğŸ¯ ä¸­å€¼æ»¤æ³¢', fontsize=12, fontweight='bold')\n",
    "    axes[0,2].axis('off')\n",
    "    \n",
    "    # åŒè¾¹æ»¤æ³¢\n",
    "    axes[1,0].imshow(cv2.cvtColor(bilateral_filtered, cv2.COLOR_BGR2RGB))\n",
    "    axes[1,0].set_title('ğŸ”„ åŒè¾¹æ»¤æ³¢', fontsize=12, fontweight='bold')\n",
    "    axes[1,0].axis('off')\n",
    "    \n",
    "    # æ‹‰æ™®æ‹‰æ–¯è¾¹ç¼˜æ£€æµ‹\n",
    "    axes[1,1].imshow(laplacian, cmap='gray')\n",
    "    axes[1,1].set_title('âš¡ æ‹‰æ™®æ‹‰æ–¯è¾¹ç¼˜æ£€æµ‹', fontsize=12, fontweight='bold')\n",
    "    axes[1,1].axis('off')\n",
    "    \n",
    "    # Sobelè¾¹ç¼˜æ£€æµ‹\n",
    "    axes[1,2].imshow(sobel_combined, cmap='gray')\n",
    "    axes[1,2].set_title('ğŸ” Sobelè¾¹ç¼˜æ£€æµ‹', fontsize=12, fontweight='bold')\n",
    "    axes[1,2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nğŸ“‹ OpenCVå¤„ç†æ€»ç»“ï¼š\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"âœ… æˆåŠŸåˆ›å»º {canvas.shape[1]}x{canvas.shape[0]} åƒç´ çš„å›¾åƒ\")\n",
    "    print(f\"âœ… æ£€æµ‹åˆ° {len(contours)} ä¸ªè½®å»“\")\n",
    "    print(f\"âœ… åº”ç”¨äº† 8 ç§ä¸åŒçš„å›¾åƒå¤„ç†æŠ€æœ¯\")\n",
    "    print(f\"âœ… ç”Ÿæˆäº† {len(contour_areas)} ä¸ªæœ‰æ•ˆè½®å»“åˆ†æ\")\n",
    "    print(f\"âœ… å›¾åƒå¹³å‡äº®åº¦: {np.mean(gray):.2f}\")\n",
    "    print(f\"âœ… å›¾åƒå¯¹æ¯”åº¦(æ ‡å‡†å·®): {np.std(gray):.2f}\")\n",
    "    \n",
    "    # 6. å®é™…åº”ç”¨ç¤ºä¾‹\n",
    "    print(\"\\nğŸ¯ å®é™…åº”ç”¨åœºæ™¯ç¤ºä¾‹...\")\n",
    "    \n",
    "    # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„æ–‡æ¡£å›¾åƒ\n",
    "    doc_img = np.ones((400, 600, 3), dtype=np.uint8) * 255  # ç™½è‰²èƒŒæ™¯\n",
    "    \n",
    "    # æ·»åŠ ä¸€äº›\"æ–‡å­—\"ï¼ˆç”¨çŸ©å½¢æ¨¡æ‹Ÿï¼‰\n",
    "    text_blocks = [\n",
    "        ((50, 50), (550, 80)),\n",
    "        ((50, 100), (500, 130)),\n",
    "        ((50, 150), (520, 180)),\n",
    "        ((50, 200), (480, 230)),\n",
    "        ((50, 280), (300, 310)),\n",
    "        ((350, 280), (550, 310))\n",
    "    ]\n",
    "    \n",
    "    for start, end in text_blocks:\n",
    "        cv2.rectangle(doc_img, start, end, (0, 0, 0), -1)\n",
    "    \n",
    "    # æ·»åŠ å™ªå£°\n",
    "    noise = np.random.normal(0, 25, doc_img.shape).astype(np.uint8)\n",
    "    noisy_doc = cv2.add(doc_img, noise)\n",
    "    \n",
    "    # æ–‡æ¡£å¤„ç†æµç¨‹\n",
    "    doc_gray = cv2.cvtColor(noisy_doc, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # å»å™ª\n",
    "    denoised = cv2.medianBlur(doc_gray, 5)\n",
    "    \n",
    "    # äºŒå€¼åŒ–\n",
    "    _, binary = cv2.threshold(denoised, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # å½¢æ€å­¦æ¸…ç†\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    cleaned = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    # æ–‡æ¡£å¤„ç†å±•ç¤º\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('ğŸ“„ æ–‡æ¡£å›¾åƒå¤„ç†æµç¨‹', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    axes[0,0].imshow(cv2.cvtColor(noisy_doc, cv2.COLOR_BGR2RGB))\n",
    "    axes[0,0].set_title('ğŸ“„ å¸¦å™ªå£°çš„æ–‡æ¡£', fontsize=12, fontweight='bold')\n",
    "    axes[0,0].axis('off')\n",
    "    \n",
    "    axes[0,1].imshow(denoised, cmap='gray')\n",
    "    axes[0,1].set_title('ğŸ§¹ å»å™ªå¤„ç†', fontsize=12, fontweight='bold')\n",
    "    axes[0,1].axis('off')\n",
    "    \n",
    "    axes[1,0].imshow(binary, cmap='gray')\n",
    "    axes[1,0].set_title('âš«âšª äºŒå€¼åŒ–', fontsize=12, fontweight='bold')\n",
    "    axes[1,0].axis('off')\n",
    "    \n",
    "    axes[1,1].imshow(cleaned, cmap='gray')\n",
    "    axes[1,1].set_title('âœ¨ å½¢æ€å­¦æ¸…ç†', fontsize=12, fontweight='bold')\n",
    "    axes[1,1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ OpenCVå¤„ç†å‡ºé”™: {e}\")\n",
    "    print(\"è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…OpenCV: pip install opencv-python\")\n",
    "\n",
    "print(\"\\nâœ… OpenCVè®¡ç®—æœºè§†è§‰ç¤ºä¾‹å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fb5479",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 6. ğŸš€ FastAPI = é«˜æ€§èƒ½Web API\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f0a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”¹ FastAPIé«˜æ€§èƒ½Web APIå¼€å‘\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# FastAPIä»£ç ç¤ºä¾‹ï¼ˆåœ¨Jupyterä¸­å±•ç¤ºï¼Œå®é™…éœ€è¦åœ¨.pyæ–‡ä»¶ä¸­è¿è¡Œï¼‰\n",
    "fastapi_code = '''\n",
    "from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "import uvicorn\n",
    "import asyncio\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# åˆ›å»ºFastAPIåº”ç”¨å®ä¾‹\n",
    "app = FastAPI(\n",
    "    title=\"ğŸš€ Vibe Coding API\",\n",
    "    description=\"ä¸€ä¸ªå±•ç¤ºFastAPIå¼ºå¤§åŠŸèƒ½çš„ç¤ºä¾‹API\",\n",
    "    version=\"1.0.0\",\n",
    "    docs_url=\"/docs\",  # Swagger UI\n",
    "    redoc_url=\"/redoc\"  # ReDoc\n",
    ")\n",
    "\n",
    "# æ·»åŠ CORSä¸­é—´ä»¶\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# æ•°æ®æ¨¡å‹å®šä¹‰\n",
    "class User(BaseModel):\n",
    "    id: Optional[int] = None\n",
    "    name: str = Field(..., min_length=1, max_length=50)\n",
    "    email: str = Field(..., regex=r'^[^@]+@[^@]+\\.[^@]+$')\n",
    "    age: int = Field(..., ge=0, le=150)\n",
    "    is_active: bool = True\n",
    "\n",
    "class UserResponse(BaseModel):\n",
    "    id: int\n",
    "    name: str\n",
    "    email: str\n",
    "    age: int\n",
    "    is_active: bool\n",
    "    created_at: datetime\n",
    "\n",
    "class Product(BaseModel):\n",
    "    id: Optional[int] = None\n",
    "    name: str\n",
    "    price: float = Field(..., gt=0)\n",
    "    description: Optional[str] = None\n",
    "    category: str\n",
    "    in_stock: bool = True\n",
    "\n",
    "# æ¨¡æ‹Ÿæ•°æ®åº“\n",
    "users_db = []\n",
    "products_db = []\n",
    "logs_db = []\n",
    "\n",
    "# ä¾èµ–æ³¨å…¥ç¤ºä¾‹\n",
    "async def get_current_time():\n",
    "    return datetime.now()\n",
    "\n",
    "# åå°ä»»åŠ¡ç¤ºä¾‹\n",
    "def log_operation(operation: str, details: str):\n",
    "    log_entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"operation\": operation,\n",
    "        \"details\": details\n",
    "    }\n",
    "    logs_db.append(log_entry)\n",
    "    print(f\"ğŸ“ è®°å½•æ“ä½œ: {operation} - {details}\")\n",
    "\n",
    "# æ ¹è·¯å¾„\n",
    "@app.get(\"/\", tags=[\"åŸºç¡€\"])\n",
    "async def root():\n",
    "    \"\"\"\n",
    "    ğŸ  APIæ ¹è·¯å¾„ - è¿”å›æ¬¢è¿ä¿¡æ¯\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"message\": \"ğŸš€ æ¬¢è¿ä½¿ç”¨ Vibe Coding APIï¼\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"docs\": \"/docs\",\n",
    "        \"features\": [\"ç”¨æˆ·ç®¡ç†\", \"äº§å“ç®¡ç†\", \"å®æ—¶æ•°æ®\", \"åå°ä»»åŠ¡\"]\n",
    "    }\n",
    "\n",
    "# å¥åº·æ£€æŸ¥\n",
    "@app.get(\"/health\", tags=[\"ç³»ç»Ÿ\"])\n",
    "async def health_check(current_time: datetime = Depends(get_current_time)):\n",
    "    \"\"\"\n",
    "    â¤ï¸ ç³»ç»Ÿå¥åº·æ£€æŸ¥\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"timestamp\": current_time,\n",
    "        \"uptime\": \"æ­£å¸¸è¿è¡Œ\",\n",
    "        \"database\": \"è¿æ¥æ­£å¸¸\"\n",
    "    }\n",
    "\n",
    "# ç”¨æˆ·ç®¡ç†API\n",
    "@app.post(\"/users/\", response_model=UserResponse, tags=[\"ç”¨æˆ·ç®¡ç†\"])\n",
    "async def create_user(\n",
    "    user: User, \n",
    "    background_tasks: BackgroundTasks,\n",
    "    current_time: datetime = Depends(get_current_time)\n",
    "):\n",
    "    \"\"\"\n",
    "    ğŸ‘¤ åˆ›å»ºæ–°ç”¨æˆ·\n",
    "    \"\"\"\n",
    "    # æ£€æŸ¥é‚®ç®±æ˜¯å¦å·²å­˜åœ¨\n",
    "    if any(u[\"email\"] == user.email for u in users_db):\n",
    "        raise HTTPException(status_code=400, detail=\"é‚®ç®±å·²å­˜åœ¨\")\n",
    "    \n",
    "    # åˆ›å»ºç”¨æˆ·\n",
    "    user_id = len(users_db) + 1\n",
    "    user_data = {\n",
    "        \"id\": user_id,\n",
    "        \"name\": user.name,\n",
    "        \"email\": user.email,\n",
    "        \"age\": user.age,\n",
    "        \"is_active\": user.is_active,\n",
    "        \"created_at\": current_time\n",
    "    }\n",
    "    users_db.append(user_data)\n",
    "    \n",
    "    # æ·»åŠ åå°ä»»åŠ¡\n",
    "    background_tasks.add_task(\n",
    "        log_operation, \n",
    "        \"CREATE_USER\", \n",
    "        f\"ç”¨æˆ· {user.name} å·²åˆ›å»º\"\n",
    "    )\n",
    "    \n",
    "    return user_data\n",
    "\n",
    "@app.get(\"/users/\", response_model=List[UserResponse], tags=[\"ç”¨æˆ·ç®¡ç†\"])\n",
    "async def get_users(skip: int = 0, limit: int = 10):\n",
    "    \"\"\"\n",
    "    ğŸ“‹ è·å–ç”¨æˆ·åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    return users_db[skip:skip + limit]\n",
    "\n",
    "@app.get(\"/users/{user_id}\", response_model=UserResponse, tags=[\"ç”¨æˆ·ç®¡ç†\"])\n",
    "async def get_user(user_id: int):\n",
    "    \"\"\"\n",
    "    ğŸ” æ ¹æ®IDè·å–ç”¨æˆ·ä¿¡æ¯\n",
    "    \"\"\"\n",
    "    user = next((u for u in users_db if u[\"id\"] == user_id), None)\n",
    "    if not user:\n",
    "        raise HTTPException(status_code=404, detail=\"ç”¨æˆ·ä¸å­˜åœ¨\")\n",
    "    return user\n",
    "\n",
    "@app.put(\"/users/{user_id}\", response_model=UserResponse, tags=[\"ç”¨æˆ·ç®¡ç†\"])\n",
    "async def update_user(user_id: int, user_update: User, background_tasks: BackgroundTasks):\n",
    "    \"\"\"\n",
    "    âœï¸ æ›´æ–°ç”¨æˆ·ä¿¡æ¯\n",
    "    \"\"\"\n",
    "    user_index = next((i for i, u in enumerate(users_db) if u[\"id\"] == user_id), None)\n",
    "    if user_index is None:\n",
    "        raise HTTPException(status_code=404, detail=\"ç”¨æˆ·ä¸å­˜åœ¨\")\n",
    "    \n",
    "    # æ›´æ–°ç”¨æˆ·æ•°æ®\n",
    "    users_db[user_index].update({\n",
    "        \"name\": user_update.name,\n",
    "        \"email\": user_update.email,\n",
    "        \"age\": user_update.age,\n",
    "        \"is_active\": user_update.is_active\n",
    "    })\n",
    "    \n",
    "    background_tasks.add_task(\n",
    "        log_operation, \n",
    "        \"UPDATE_USER\", \n",
    "        f\"ç”¨æˆ· {user_id} ä¿¡æ¯å·²æ›´æ–°\"\n",
    "    )\n",
    "    \n",
    "    return users_db[user_index]\n",
    "\n",
    "@app.delete(\"/users/{user_id}\", tags=[\"ç”¨æˆ·ç®¡ç†\"])\n",
    "async def delete_user(user_id: int, background_tasks: BackgroundTasks):\n",
    "    \"\"\"\n",
    "    ğŸ—‘ï¸ åˆ é™¤ç”¨æˆ·\n",
    "    \"\"\"\n",
    "    user_index = next((i for i, u in enumerate(users_db) if u[\"id\"] == user_id), None)\n",
    "    if user_index is None:\n",
    "        raise HTTPException(status_code=404, detail=\"ç”¨æˆ·ä¸å­˜åœ¨\")\n",
    "    \n",
    "    deleted_user = users_db.pop(user_index)\n",
    "    background_tasks.add_task(\n",
    "        log_operation, \n",
    "        \"DELETE_USER\", \n",
    "        f\"ç”¨æˆ· {deleted_user['name']} å·²åˆ é™¤\"\n",
    "    )\n",
    "    \n",
    "    return {\"message\": f\"ç”¨æˆ· {deleted_user['name']} å·²æˆåŠŸåˆ é™¤\"}\n",
    "\n",
    "# äº§å“ç®¡ç†API\n",
    "@app.post(\"/products/\", tags=[\"äº§å“ç®¡ç†\"])\n",
    "async def create_product(product: Product, background_tasks: BackgroundTasks):\n",
    "    \"\"\"\n",
    "    ğŸ›ï¸ åˆ›å»ºæ–°äº§å“\n",
    "    \"\"\"\n",
    "    product_id = len(products_db) + 1\n",
    "    product_data = {\n",
    "        \"id\": product_id,\n",
    "        \"name\": product.name,\n",
    "        \"price\": product.price,\n",
    "        \"description\": product.description,\n",
    "        \"category\": product.category,\n",
    "        \"in_stock\": product.in_stock,\n",
    "        \"created_at\": datetime.now()\n",
    "    }\n",
    "    products_db.append(product_data)\n",
    "    \n",
    "    background_tasks.add_task(\n",
    "        log_operation, \n",
    "        \"CREATE_PRODUCT\", \n",
    "        f\"äº§å“ {product.name} å·²åˆ›å»º\"\n",
    "    )\n",
    "    \n",
    "    return product_data\n",
    "\n",
    "@app.get(\"/products/\", tags=[\"äº§å“ç®¡ç†\"])\n",
    "async def get_products(category: Optional[str] = None, in_stock: Optional[bool] = None):\n",
    "    \"\"\"\n",
    "    ğŸ“¦ è·å–äº§å“åˆ—è¡¨ï¼ˆæ”¯æŒç­›é€‰ï¼‰\n",
    "    \"\"\"\n",
    "    filtered_products = products_db\n",
    "    \n",
    "    if category:\n",
    "        filtered_products = [p for p in filtered_products if p[\"category\"] == category]\n",
    "    \n",
    "    if in_stock is not None:\n",
    "        filtered_products = [p for p in filtered_products if p[\"in_stock\"] == in_stock]\n",
    "    \n",
    "    return filtered_products\n",
    "\n",
    "# å®æ—¶æ•°æ®API\n",
    "@app.get(\"/stats/realtime\", tags=[\"ç»Ÿè®¡æ•°æ®\"])\n",
    "async def get_realtime_stats():\n",
    "    \"\"\"\n",
    "    ğŸ“Š è·å–å®æ—¶ç»Ÿè®¡æ•°æ®\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"timestamp\": datetime.now(),\n",
    "        \"total_users\": len(users_db),\n",
    "        \"active_users\": len([u for u in users_db if u[\"is_active\"]]),\n",
    "        \"total_products\": len(products_db),\n",
    "        \"products_in_stock\": len([p for p in products_db if p[\"in_stock\"]]),\n",
    "        \"total_operations\": len(logs_db)\n",
    "    }\n",
    "\n",
    "# å¼‚æ­¥æ•°æ®å¤„ç†\n",
    "@app.get(\"/data/process\", tags=[\"æ•°æ®å¤„ç†\"])\n",
    "async def process_data_async():\n",
    "    \"\"\"\n",
    "    âš¡ å¼‚æ­¥æ•°æ®å¤„ç†ç¤ºä¾‹\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # æ¨¡æ‹Ÿå¼‚æ­¥æ•°æ®å¤„ç†\n",
    "    await asyncio.sleep(1)  # æ¨¡æ‹Ÿè€—æ—¶æ“ä½œ\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"completed\",\n",
    "        \"processing_time\": f\"{processing_time:.2f}ç§’\",\n",
    "        \"processed_items\": 1000,\n",
    "        \"timestamp\": datetime.now()\n",
    "    }\n",
    "\n",
    "# æ–‡ä»¶ä¸Šä¼ ç¤ºä¾‹\n",
    "@app.post(\"/upload/\", tags=[\"æ–‡ä»¶æ“ä½œ\"])\n",
    "async def upload_file(background_tasks: BackgroundTasks):\n",
    "    \"\"\"\n",
    "    ğŸ“ æ–‡ä»¶ä¸Šä¼ ç¤ºä¾‹ï¼ˆæ¨¡æ‹Ÿï¼‰\n",
    "    \"\"\"\n",
    "    # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šå¤„ç†æ–‡ä»¶ä¸Šä¼ \n",
    "    file_info = {\n",
    "        \"filename\": \"example.jpg\",\n",
    "        \"size\": \"2.5MB\",\n",
    "        \"uploaded_at\": datetime.now(),\n",
    "        \"status\": \"success\"\n",
    "    }\n",
    "    \n",
    "    background_tasks.add_task(\n",
    "        log_operation, \n",
    "        \"FILE_UPLOAD\", \n",
    "        f\"æ–‡ä»¶ {file_info['filename']} ä¸Šä¼ æˆåŠŸ\"\n",
    "    )\n",
    "    \n",
    "    return file_info\n",
    "\n",
    "# æ“ä½œæ—¥å¿—API\n",
    "@app.get(\"/logs/\", tags=[\"ç³»ç»Ÿ\"])\n",
    "async def get_operation_logs(limit: int = 50):\n",
    "    \"\"\"\n",
    "    ğŸ“ è·å–æ“ä½œæ—¥å¿—\n",
    "    \"\"\"\n",
    "    return logs_db[-limit:]\n",
    "\n",
    "# WebSocketæ”¯æŒç¤ºä¾‹\n",
    "@app.websocket(\"/ws\")\n",
    "async def websocket_endpoint(websocket):\n",
    "    \"\"\"\n",
    "    ğŸ”Œ WebSocketè¿æ¥ç¤ºä¾‹\n",
    "    \"\"\"\n",
    "    await websocket.accept()\n",
    "    try:\n",
    "        while True:\n",
    "            # å‘é€å®æ—¶æ•°æ®\n",
    "            stats = {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"users\": len(users_db),\n",
    "                \"products\": len(products_db),\n",
    "                \"random_value\": time.time() % 100\n",
    "            }\n",
    "            await websocket.send_text(json.dumps(stats))\n",
    "            await asyncio.sleep(5)  # æ¯5ç§’å‘é€ä¸€æ¬¡\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸš€ å¯åŠ¨ FastAPI æœåŠ¡å™¨...\")\n",
    "    print(\"ğŸ“– APIæ–‡æ¡£: http://localhost:8000/docs\")\n",
    "    print(\"ğŸ“š ReDocæ–‡æ¡£: http://localhost:8000/redoc\")\n",
    "    \n",
    "    uvicorn.run(\n",
    "        app, \n",
    "        host=\"0.0.0.0\", \n",
    "        port=8000,\n",
    "        reload=True,  # å¼€å‘æ¨¡å¼ï¼Œä»£ç æ›´æ”¹æ—¶è‡ªåŠ¨é‡è½½\n",
    "        log_level=\"info\"\n",
    "    )\n",
    "'''\n",
    "\n",
    "print(\"FastAPIå®Œæ•´ç¤ºä¾‹ä»£ç :\")\n",
    "print(\"=\" * 80)\n",
    "print(fastapi_code)\n",
    "\n",
    "print(\"\\nğŸ› ï¸ ä½¿ç”¨è¯´æ˜:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"1. ğŸ“ å°†ä¸Šè¿°ä»£ç ä¿å­˜ä¸º 'main.py'\")\n",
    "print(\"2. ğŸ“¦ å®‰è£…ä¾èµ–: pip install fastapi uvicorn\")\n",
    "print(\"3. ğŸš€ è¿è¡ŒæœåŠ¡: uvicorn main:app --reload\")\n",
    "print(\"4. ğŸŒ è®¿é—®APIæ–‡æ¡£: http://localhost:8000/docs\")\n",
    "print(\"5. ğŸ“š æŸ¥çœ‹ReDoc: http://localhost:8000/redoc\")\n",
    "\n",
    "print(\"\\nğŸ¯ APIåŠŸèƒ½ç‰¹æ€§:\")\n",
    "print(\"=\" * 50)\n",
    "features = [\n",
    "    \"âœ… è‡ªåŠ¨APIæ–‡æ¡£ç”Ÿæˆ (Swagger UI + ReDoc)\",\n",
    "    \"âœ… æ•°æ®éªŒè¯å’Œåºåˆ—åŒ– (Pydantic)\",\n",
    "    \"âœ… å¼‚æ­¥æ”¯æŒå’Œé«˜æ€§èƒ½\",\n",
    "    \"âœ… ä¾èµ–æ³¨å…¥ç³»ç»Ÿ\",\n",
    "    \"âœ… åå°ä»»åŠ¡å¤„ç†\",\n",
    "    \"âœ… CORSä¸­é—´ä»¶æ”¯æŒ\",\n",
    "    \"âœ… WebSocketå®æ—¶é€šä¿¡\",\n",
    "    \"âœ… é”™è¯¯å¤„ç†å’ŒçŠ¶æ€ç \",\n",
    "    \"âœ… æŸ¥è¯¢å‚æ•°å’Œè·¯å¾„å‚æ•°\",\n",
    "    \"âœ… è¯·æ±‚ä½“éªŒè¯\",\n",
    "    \"âœ… å“åº”æ¨¡å‹å®šä¹‰\",\n",
    "    \"âœ… æ ‡ç­¾åˆ†ç»„å’Œæ–‡æ¡£ç»„ç»‡\"\n",
    "]\n",
    "\n",
    "for feature in features:\n",
    "    print(f\"  {feature}\")\n",
    "\n",
    "print(\"\\nğŸ“¡ APIç«¯ç‚¹ç¤ºä¾‹:\")\n",
    "print(\"=\" * 50)\n",
    "endpoints = [\n",
    "    (\"GET\", \"/\", \"è·å–APIæ¬¢è¿ä¿¡æ¯\"),\n",
    "    (\"GET\", \"/health\", \"å¥åº·æ£€æŸ¥\"),\n",
    "    (\"POST\", \"/users/\", \"åˆ›å»ºç”¨æˆ·\"),\n",
    "    (\"GET\", \"/users/\", \"è·å–ç”¨æˆ·åˆ—è¡¨\"),\n",
    "    (\"GET\", \"/users/{user_id}\", \"è·å–å•ä¸ªç”¨æˆ·\"),\n",
    "    (\"PUT\", \"/users/{user_id}\", \"æ›´æ–°ç”¨æˆ·\"),\n",
    "    (\"DELETE\", \"/users/{user_id}\", \"åˆ é™¤ç”¨æˆ·\"),\n",
    "    (\"POST\", \"/products/\", \"åˆ›å»ºäº§å“\"),\n",
    "    (\"GET\", \"/products/\", \"è·å–äº§å“åˆ—è¡¨\"),\n",
    "    (\"GET\", \"/stats/realtime\", \"å®æ—¶ç»Ÿè®¡æ•°æ®\"),\n",
    "    (\"GET\", \"/data/process\", \"å¼‚æ­¥æ•°æ®å¤„ç†\"),\n",
    "    (\"POST\", \"/upload/\", \"æ–‡ä»¶ä¸Šä¼ \"),\n",
    "    (\"GET\", \"/logs/\", \"æ“ä½œæ—¥å¿—\"),\n",
    "    (\"WebSocket\", \"/ws\", \"å®æ—¶æ•°æ®æ¨é€\")\n",
    "]\n",
    "\n",
    "for method, endpoint, description in endpoints:\n",
    "    print(f\"  {method:10} {endpoint:20} - {description}\")\n",
    "\n",
    "print(\"\\nğŸ”§ æµ‹è¯•APIçš„æ–¹æ³•:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"1. ğŸŒ æµè§ˆå™¨è®¿é—®: http://localhost:8000/docs\")\n",
    "print(\"2. ğŸ“± ä½¿ç”¨curlå‘½ä»¤:\")\n",
    "print(\"   curl -X GET http://localhost:8000/\")\n",
    "print(\"   curl -X POST http://localhost:8000/users/ \\\\\")\n",
    "print(\"        -H 'Content-Type: application/json' \\\\\")\n",
    "print(\"        -d '{\\\"name\\\":\\\"å¼ ä¸‰\\\",\\\"email\\\":\\\"zhang@example.com\\\",\\\"age\\\":25}'\")\n",
    "print(\"3. ğŸ” ä½¿ç”¨Postmanæˆ–å…¶ä»–APIæµ‹è¯•å·¥å…·\")\n",
    "print(\"4. ğŸ“Š Python requestsåº“:\")\n",
    "\n",
    "requests_example = '''\n",
    "import requests\n",
    "\n",
    "# è·å–APIä¿¡æ¯\n",
    "response = requests.get(\"http://localhost:8000/\")\n",
    "print(response.json())\n",
    "\n",
    "# åˆ›å»ºç”¨æˆ·\n",
    "user_data = {\n",
    "    \"name\": \"æå››\",\n",
    "    \"email\": \"lisi@example.com\", \n",
    "    \"age\": 30\n",
    "}\n",
    "response = requests.post(\"http://localhost:8000/users/\", json=user_data)\n",
    "print(response.json())\n",
    "\n",
    "# è·å–ç”¨æˆ·åˆ—è¡¨\n",
    "response = requests.get(\"http://localhost:8000/users/\")\n",
    "print(response.json())\n",
    "'''\n",
    "\n",
    "print(requests_example)\n",
    "\n",
    "print(\"\\nâœ… FastAPIç¤ºä¾‹å±•ç¤ºå®Œæˆï¼\")\n",
    "print(\"ğŸ’¡ è¿™ä¸ªAPIå±•ç¤ºäº†ç°ä»£Webå¼€å‘çš„æœ€ä½³å®è·µï¼ŒåŒ…æ‹¬ç±»å‹æç¤ºã€è‡ªåŠ¨æ–‡æ¡£ã€å¼‚æ­¥å¤„ç†ç­‰ç‰¹æ€§ã€‚\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "play",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
