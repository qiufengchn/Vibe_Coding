import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, random_split
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_percentage_error

print("ğŸ”¹ ä½¿ç”¨PyTorchæ„å»ºæœºå™¨å­¦ä¹ æ¨¡å‹")
print("=" * 50)

# æ£€æŸ¥PyTorchç‰ˆæœ¬å’ŒGPU
print(f"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
print(f"âœ… å¯ç”¨GPU: {'å¯ç”¨' if torch.cuda.is_available() else 'ä¸å¯ç”¨'}")

# 1. åˆ›å»ºæˆ¿ä»·é¢„æµ‹æ•°æ®é›†ï¼ˆå›å½’é—®é¢˜ï¼‰
print("\nğŸ  åˆ›å»ºæˆ¿ä»·é¢„æµ‹æ•°æ®é›†...")
np.random.seed(42)
n_samples = 2000

# ç”Ÿæˆç‰¹å¾æ•°æ®
house_data = pd.DataFrame({
    'é¢ç§¯': np.random.normal(120, 40, n_samples),  # å¹³æ–¹ç±³
    'æˆ¿é—´æ•°': np.random.randint(1, 6, n_samples),   # æˆ¿é—´æ•°é‡
    'æ¥¼å±‚': np.random.randint(1, 31, n_samples),    # æ¥¼å±‚
    'æˆ¿é¾„': np.random.randint(0, 30, n_samples),    # æˆ¿é¾„ï¼ˆå¹´ï¼‰
    'è·ç¦»åœ°é“': np.random.exponential(2, n_samples),  # è·ç¦»åœ°é“ç«™ï¼ˆå…¬é‡Œï¼‰
    'å­¦åŒºæˆ¿': np.random.choice([0, 1], n_samples, p=[0.7, 0.3]),  # æ˜¯å¦å­¦åŒºæˆ¿
    'è£…ä¿®æƒ…å†µ': np.random.choice([0, 1, 2], n_samples, p=[0.3, 0.5, 0.2])  # 0ç®€è£… 1ç²¾è£… 2è±ªè£…
})

# ç¡®ä¿æ•°æ®åˆç†æ€§
house_data['é¢ç§¯'] = np.clip(house_data['é¢ç§¯'], 30, 300)
house_data['è·ç¦»åœ°é“'] = np.clip(house_data['è·ç¦»åœ°é“'], 0.1, 10)

# ç”Ÿæˆç›®æ ‡å˜é‡ï¼ˆæˆ¿ä»·ï¼‰- åŸºäºç‰¹å¾çš„å¤æ‚å…³ç³»
base_price = (
    house_data['é¢ç§¯'] * 500 +                    # é¢ç§¯å½±å“
    house_data['æˆ¿é—´æ•°'] * 15000 +                # æˆ¿é—´æ•°å½±å“
    (31 - house_data['æ¥¼å±‚']) * 2000 +            # æ¥¼å±‚å½±å“ï¼ˆé«˜æ¥¼å±‚æ›´è´µï¼‰
    (30 - house_data['æˆ¿é¾„']) * 3000 +            # æˆ¿é¾„å½±å“ï¼ˆæ–°æˆ¿æ›´è´µï¼‰
    (10 - house_data['è·ç¦»åœ°é“']) * 8000 +        # åœ°é“è·ç¦»å½±å“
    house_data['å­¦åŒºæˆ¿'] * 200000 +               # å­¦åŒºæˆ¿åŠ æˆ
    house_data['è£…ä¿®æƒ…å†µ'] * 50000                # è£…ä¿®æƒ…å†µå½±å“
)

# æ·»åŠ å™ªå£°
noise = np.random.normal(0, 50000, n_samples)
house_data['ä»·æ ¼'] = base_price + noise
house_data['ä»·æ ¼'] = np.clip(house_data['ä»·æ ¼'], 100000, 2000000)  # é™åˆ¶ä»·æ ¼èŒƒå›´

print("æ•°æ®é›†åŸºæœ¬ä¿¡æ¯ï¼š")
print(house_data.describe())
print(f"\næ•°æ®é›†å½¢çŠ¶: {house_data.shape}")

# 2. æ•°æ®é¢„å¤„ç†
print("\nğŸ”§ æ•°æ®é¢„å¤„ç†...")
X = house_data.drop('ä»·æ ¼', axis=1).values
y = house_data['ä»·æ ¼'].values.reshape(-1, 1)

# æ•°æ®åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ç‰¹å¾æ ‡å‡†åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# è½¬æ¢ä¸ºPyTorchå¼ é‡
X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

# åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

# åˆ›å»ºéªŒè¯é›†
train_size = int(0.8 * len(train_dataset))
val_size = len(train_dataset) - train_size
train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])

batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
print(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")
print(f"æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")

# 3. æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹
print("\nğŸ§  æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹...")

class HousePriceModel(nn.Module):
    def __init__(self, input_size):
        super(HousePriceModel, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_size, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1)
        )
    
    def forward(self, x):
        return self.network(x)

# åˆ›å»ºæ¨¡å‹
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = HousePriceModel(X_train_scaled.shape[1]).to(device)
print("æ¨¡å‹ç»“æ„ï¼š")
print(model)

# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# è®­ç»ƒæ¨¡å‹
print("\nğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
num_epochs = 100
patience = 10
best_val_loss = float('inf')
counter = 0

# å­˜å‚¨è®­ç»ƒå†å²
history = {
    'train_loss': [],
    'val_loss': [],
    'train_mae': [],
    'val_mae': []
}

for epoch in range(num_epochs):
    # è®­ç»ƒé˜¶æ®µ
    model.train()
    train_loss = 0.0
    train_mae = 0.0
    for inputs, targets in train_loader:
        inputs, targets = inputs.to(device), targets.to(device)
        
        # å‰å‘ä¼ æ’­
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        
        # åå‘ä¼ æ’­å’Œä¼˜åŒ–
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # è®¡ç®—ç»Ÿè®¡é‡
        train_loss += loss.item()
        train_mae += torch.mean(torch.abs(outputs - targets)).item()
    
    # éªŒè¯é˜¶æ®µ
    model.eval()
    val_loss = 0.0
    val_mae = 0.0
    with torch.no_grad():
        for inputs, targets in val_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            
            val_loss += criterion(outputs, targets).item()
            val_mae += torch.mean(torch.abs(outputs - targets)).item()
    
    # è®¡ç®—å¹³å‡æŸå¤±
    train_loss /= len(train_loader)
    val_loss /= len(val_loader)
    train_mae /= len(train_loader)
    val_mae /= len(val_loader)
    
    # å­˜å‚¨å†å²
    history['train_loss'].append(train_loss)
    history['val_loss'].append(val_loss)
    history['train_mae'].append(train_mae)
    history['val_mae'].append(val_mae)
    
    # æ‰“å°è¿›åº¦
    print(f"Epoch [{epoch+1}/{num_epochs}], "
          f"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, "
          f"Train MAE: {train_mae:.2f}, Val MAE: {val_mae:.2f}")
    
    # æ—©åœæœºåˆ¶
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        counter = 0
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        torch.save(model.state_dict(), 'best_model.pth')
    else:
        counter += 1
        if counter >= patience:
            print(f"æ—©åœåœ¨ç¬¬ {epoch+1} è½®")
            break

# åŠ è½½æœ€ä½³æ¨¡å‹
model.load_state_dict(torch.load('best_model.pth'))
model.eval()

# 4. æ¨¡å‹è¯„ä¼°
print("\nğŸ“Š æ¨¡å‹è¯„ä¼°...")

# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
test_loss = 0.0
test_mae = 0.0
all_targets = []
all_predictions = []

with torch.no_grad():
    for inputs, targets in test_loader:
        inputs, targets = inputs.to(device), targets.to(device)
        outputs = model(inputs)
        
        test_loss += criterion(outputs, targets).item()
        test_mae += torch.mean(torch.abs(outputs - targets)).item()
        
        all_targets.append(targets.cpu().numpy())
        all_predictions.append(outputs.cpu().numpy())

# è®¡ç®—å¹³å‡æŸå¤±
test_loss /= len(test_loader)
test_mae /= len(test_loader)

# åˆå¹¶é¢„æµ‹ç»“æœ
all_targets = np.concatenate(all_targets)
all_predictions = np.concatenate(all_predictions)

# è®¡ç®—æ›´å¤šæŒ‡æ ‡
r2 = r2_score(all_targets, all_predictions)
mape = mean_absolute_percentage_error(all_targets, all_predictions)

print(f"æµ‹è¯•é›†æŸå¤± (MSE): {test_loss:.2f}")
print(f"æµ‹è¯•é›†å¹³å‡ç»å¯¹è¯¯å·® (MAE): {test_mae:.2f}")
print(f"RÂ² åˆ†æ•°: {r2:.4f}")
print(f"å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·® (MAPE): {mape:.4f}")

# 5. å¯è§†åŒ–ç»“æœ
print("\nğŸ“ˆ ç»“æœå¯è§†åŒ–...")

fig, axes = plt.subplots(2, 2, figsize=(15, 12))
fig.suptitle('ğŸ¤– PyTorchæˆ¿ä»·é¢„æµ‹æ¨¡å‹åˆ†æ', fontsize=16, fontweight='bold')

# å­å›¾1ï¼šè®­ç»ƒå†å²
axes[0,0].plot(history['train_loss'], label='è®­ç»ƒæŸå¤±', color='blue')
axes[0,0].plot(history['val_loss'], label='éªŒè¯æŸå¤±', color='red')
axes[0,0].set_title('ğŸ“‰ æ¨¡å‹è®­ç»ƒå†å²', fontsize=12, fontweight='bold')
axes[0,0].set_xlabel('è½®æ¬¡')
axes[0,0].set_ylabel('æŸå¤±')
axes[0,0].legend()
axes[0,0].grid(True, alpha=0.3)

# å­å›¾2ï¼šMAEå†å²
axes[0,1].plot(history['train_mae'], label='è®­ç»ƒMAE', color='green')
axes[0,1].plot(history['val_mae'], label='éªŒè¯MAE', color='orange')
axes[0,1].set_title('ğŸ“Š å¹³å‡ç»å¯¹è¯¯å·®å†å²', fontsize=12, fontweight='bold')
axes[0,1].set_xlabel('è½®æ¬¡')
axes[0,1].set_ylabel('MAE')
axes[0,1].legend()
axes[0,1].grid(True, alpha=0.3)

# å­å›¾3ï¼šé¢„æµ‹vså®é™…
axes[1,0].scatter(all_targets, all_predictions, alpha=0.6, color='purple')
axes[1,0].plot([all_targets.min(), all_targets.max()], 
              [all_targets.min(), all_targets.max()], 'r--', lw=2)
axes[1,0].set_title('ğŸ¯ é¢„æµ‹å€¼ vs å®é™…å€¼', fontsize=12, fontweight='bold')
axes[1,0].set_xlabel('å®é™…æˆ¿ä»·')
axes[1,0].set_ylabel('é¢„æµ‹æˆ¿ä»·')
axes[1,0].grid(True, alpha=0.3)

# å­å›¾4ï¼šæ®‹å·®åˆ†å¸ƒ - ä¿®å¤é”™è¯¯ï¼šç¡®ä¿æ˜¯ä¸€ç»´æ•°ç»„
residuals = (all_targets - all_predictions).flatten()  # å±•å¹³æ•°ç»„
axes[1,1].hist(residuals, bins=30, alpha=0.7, color='cyan', edgecolor='black')
axes[1,1].set_title('ğŸ“Š æ®‹å·®åˆ†å¸ƒ', fontsize=12, fontweight='bold')
axes[1,1].set_xlabel('æ®‹å·®')
axes[1,1].set_ylabel('é¢‘æ¬¡')
axes[1,1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('pytorch_house_price_analysis.png')
plt.show()

# 6. ç‰¹å¾é‡è¦æ€§åˆ†æ
print("\nğŸ” ç‰¹å¾é‡è¦æ€§åˆ†æ...")

# è·å–ç¬¬ä¸€å±‚æƒé‡
first_layer = model.network[0]
weights = first_layer.weight.data.cpu().numpy()
feature_importance = np.abs(weights).mean(axis=0)

feature_names = house_data.drop('ä»·æ ¼', axis=1).columns
importance_df = pd.DataFrame({
    'ç‰¹å¾': feature_names,
    'é‡è¦æ€§': feature_importance
}).sort_values('é‡è¦æ€§', ascending=False)

print("ç‰¹å¾é‡è¦æ€§æ’åºï¼š")
print(importance_df)

# ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§
plt.figure(figsize=(10, 6))
bars = plt.bar(importance_df['ç‰¹å¾'], importance_df['é‡è¦æ€§'], 
               color='lightblue', alpha=0.8, edgecolor='navy')
plt.title('ğŸ¯ ç‰¹å¾é‡è¦æ€§åˆ†æ', fontsize=14, fontweight='bold')
plt.xlabel('ç‰¹å¾')
plt.ylabel('é‡è¦æ€§åˆ†æ•°')
plt.xticks(rotation=45)
plt.grid(True, alpha=0.3, axis='y')

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for bar, value in zip(bars, importance_df['é‡è¦æ€§']):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, 
            f'{value:.3f}', ha='center', va='bottom')

plt.tight_layout()
plt.savefig('feature_importance.png')
plt.show()

# 7. å®é™…é¢„æµ‹ç¤ºä¾‹
print("\nğŸ¡ å®é™…é¢„æµ‹ç¤ºä¾‹...")

# åˆ›å»ºå‡ ä¸ªç¤ºä¾‹æˆ¿å­
sample_houses = pd.DataFrame({
    'é¢ç§¯': [90, 130, 200],
    'æˆ¿é—´æ•°': [2, 3, 4],
    'æ¥¼å±‚': [15, 8, 3],
    'æˆ¿é¾„': [5, 15, 2],
    'è·ç¦»åœ°é“': [0.8, 2.5, 1.2],
    'å­¦åŒºæˆ¿': [1, 0, 1],
    'è£…ä¿®æƒ…å†µ': [2, 1, 2]
})

# æ ‡å‡†åŒ–
sample_scaled = scaler.transform(sample_houses)
sample_tensor = torch.tensor(sample_scaled, dtype=torch.float32).to(device)

# é¢„æµ‹
with torch.no_grad():
    predictions = model(sample_tensor).cpu().numpy().flatten()

print("ç¤ºä¾‹æˆ¿å­é¢„æµ‹ç»“æœï¼š")
for i, (_, house) in enumerate(sample_houses.iterrows()):
    pred_price = predictions[i]
    print(f"\næˆ¿å­ {i+1}:")
    print(f"  â€¢ é¢ç§¯: {house['é¢ç§¯']}ã¡")
    print(f"  â€¢ æˆ¿é—´æ•°: {house['æˆ¿é—´æ•°']}å®¤")
    print(f"  â€¢ æ¥¼å±‚: {house['æ¥¼å±‚']}å±‚")
    print(f"  â€¢ æˆ¿é¾„: {house['æˆ¿é¾„']}å¹´")
    print(f"  â€¢ è·ç¦»åœ°é“: {house['è·ç¦»åœ°é“']:.1f}km")
    print(f"  â€¢ å­¦åŒºæˆ¿: {'æ˜¯' if house['å­¦åŒºæˆ¿'] else 'å¦'}")
    print(f"  â€¢ è£…ä¿®: {['ç®€è£…', 'ç²¾è£…', 'è±ªè£…'][house['è£…ä¿®æƒ…å†µ']]}")
    print(f"  ğŸ·ï¸ é¢„æµ‹ä»·æ ¼: Â¥{pred_price:,.0f}")

print("\nâœ… PyTorchæœºå™¨å­¦ä¹ ç¤ºä¾‹å®Œæˆï¼")
